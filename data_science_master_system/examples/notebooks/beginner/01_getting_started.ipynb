{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Getting Started with Data Science Master System\n",
                "\n",
                "Welcome to the **Data Science Master System** - a comprehensive, production-ready framework for data science and machine learning.\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "In this notebook, you'll learn:\n",
                "1. How to install and import the system\n",
                "2. Basic data loading and exploration\n",
                "3. Your first machine learning pipeline\n",
                "4. Making predictions\n",
                "\n",
                "**Prerequisites**: Basic Python knowledge\n",
                "\n",
                "**Time Required**: ~15 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installation\n",
                "\n",
                "First, let's install the Data Science Master System:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install the package (uncomment if needed)\n",
                "# !pip install data-science-master-system\n",
                "\n",
                "# Or install from local source\n",
                "import sys\n",
                "sys.path.insert(0, '../../')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Import the System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import main components\n",
                "from data_science_master_system import (\n",
                "    Pipeline,\n",
                "    DataLoader,\n",
                "    FeatureFactory,\n",
                "    Evaluator,\n",
                "    Plotter,\n",
                ")\n",
                "\n",
                "# Standard libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"âœ… All imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Sample Data\n",
                "\n",
                "Let's load a customer churn dataset - this is a classification problem where we predict if a customer will leave."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Method 1: Using DataLoader (recommended)\n",
                "loader = DataLoader()\n",
                "df = loader.read('../data/csv/customer_churn.csv')\n",
                "\n",
                "# Check the data\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Columns: {list(df.columns)}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick data info\n",
                "print(\"\\nðŸ“Š Data Summary:\")\n",
                "print(f\"  â€¢ Total customers: {len(df)}\")\n",
                "print(f\"  â€¢ Churned customers: {df['churn'].sum()} ({df['churn'].mean()*100:.1f}%)\")\n",
                "print(f\"  â€¢ Features: {len(df.columns) - 1}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Quick Data Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize churn distribution\n",
                "plotter = Plotter(style='default')\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "# Churn distribution\n",
                "df['churn'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
                "axes[0].set_title('Churn Distribution')\n",
                "axes[0].set_xticklabels(['No Churn', 'Churn'], rotation=0)\n",
                "\n",
                "# Age distribution\n",
                "df['age'].hist(bins=20, ax=axes[1], color='steelblue')\n",
                "axes[1].set_title('Age Distribution')\n",
                "\n",
                "# Monthly charges\n",
                "df['monthly_charges'].hist(bins=20, ax=axes[2], color='coral')\n",
                "axes[2].set_title('Monthly Charges')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build Your First Pipeline\n",
                "\n",
                "Now let's create a machine learning pipeline with just **3 lines of code**!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove ID column (not useful for prediction)\n",
                "df_ml = df.drop(columns=['customer_id'])\n",
                "\n",
                "# Create and train pipeline - JUST 3 LINES!\n",
                "pipeline = Pipeline.auto_detect(df_ml, target='churn')  # Auto-detect problem type\n",
                "pipeline.fit()  # Train the model\n",
                "\n",
                "print(\"âœ… Pipeline trained successfully!\")\n",
                "print(f\"  â€¢ Problem type: {pipeline.problem_type}\")\n",
                "print(f\"  â€¢ Model: {pipeline.model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluate the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data for proper evaluation\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X = df_ml.drop(columns=['churn'])\n",
                "y = df_ml['churn']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Retrain on training data only\n",
                "pipeline_proper = Pipeline(\n",
                "    problem_type='classification',\n",
                "    model_name='random_forest',\n",
                "    auto_preprocess=True\n",
                ")\n",
                "pipeline_proper.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate on test set\n",
                "metrics = pipeline_proper.evaluate(X_test, y_test)\n",
                "\n",
                "print(\"\\nðŸ“ˆ Model Performance:\")\n",
                "for metric, value in metrics.items():\n",
                "    print(f\"  â€¢ {metric}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Make Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions on test data\n",
                "predictions = pipeline_proper.predict(X_test)\n",
                "probabilities = pipeline_proper.predict_proba(X_test)\n",
                "\n",
                "# Show sample predictions\n",
                "results = pd.DataFrame({\n",
                "    'Actual': y_test.values[:10],\n",
                "    'Predicted': predictions[:10],\n",
                "    'Churn Probability': probabilities[:10, 1].round(3)\n",
                "})\n",
                "\n",
                "print(\"\\nðŸ”® Sample Predictions:\")\n",
                "display(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Feature Importance\n",
                "\n",
                "Let's see which features are most important for predicting churn:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "importance_df = pipeline_proper.feature_importance(top_n=10)\n",
                "\n",
                "# Plot\n",
                "fig = plotter.feature_importance(importance_df, title='Top 10 Features for Churn Prediction')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Your Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained pipeline\n",
                "pipeline_proper.save('my_first_model.joblib')\n",
                "print(\"âœ… Model saved!\")\n",
                "\n",
                "# Later, you can load it:\n",
                "# loaded_pipeline = Pipeline.load('my_first_model.joblib')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ‰ Congratulations!\n",
                "\n",
                "You've just:\n",
                "- âœ… Loaded data using DataLoader\n",
                "- âœ… Explored data with visualizations\n",
                "- âœ… Built a machine learning pipeline\n",
                "- âœ… Evaluated model performance\n",
                "- âœ… Made predictions\n",
                "- âœ… Analyzed feature importance\n",
                "- âœ… Saved your model\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "Continue learning with:\n",
                "1. **02_data_loading_and_exploration.ipynb** - Deep dive into data handling\n",
                "2. **03_feature_engineering.ipynb** - Create powerful features\n",
                "3. **04_model_comparison.ipynb** - Compare multiple models"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}