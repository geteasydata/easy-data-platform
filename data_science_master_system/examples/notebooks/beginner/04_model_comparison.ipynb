{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèÜ Model Comparison and Selection\n",
                "\n",
                "Learn how to compare multiple models and select the best one.\n",
                "\n",
                "## Topics Covered\n",
                "1. Using ModelFactory to create models\n",
                "2. Comparing multiple algorithms\n",
                "3. Cross-validation\n",
                "4. Statistical significance testing\n",
                "5. Generating leaderboards\n",
                "\n",
                "**Time Required**: ~25 minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '../../')\n",
                "\n",
                "from data_science_master_system import (\n",
                "    DataLoader, Pipeline, ModelFactory, AutoModelSelector,\n",
                "    ClassificationModel, Evaluator, Plotter,\n",
                ")\n",
                "from data_science_master_system.evaluation import ModelComparison, calculate_metrics\n",
                "from sklearn.model_selection import train_test_split\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "loader = DataLoader()\n",
                "df = loader.read('../data/csv/customer_churn.csv')\n",
                "\n",
                "# Prepare features\n",
                "df_ml = df.drop(columns=['customer_id'])\n",
                "\n",
                "# Encode categorical columns\n",
                "df_encoded = pd.get_dummies(df_ml, drop_first=True)\n",
                "\n",
                "# Split features and target\n",
                "X = df_encoded.drop(columns=['churn'])\n",
                "y = df_encoded['churn']\n",
                "\n",
                "# Train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Training set: {X_train.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Available Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all available models\n",
                "classification_models = ModelFactory.list_available('classification')\n",
                "regression_models = ModelFactory.list_available('regression')\n",
                "\n",
                "print(\"üìã Available Classification Models:\")\n",
                "for m in classification_models:\n",
                "    print(f\"  ‚Ä¢ {m}\")\n",
                "\n",
                "print(\"\\nüìã Available Regression Models:\")\n",
                "for m in regression_models:\n",
                "    print(f\"  ‚Ä¢ {m}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create and Train Multiple Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define models to compare\n",
                "model_configs = [\n",
                "    ('Random Forest', 'random_forest', {'n_estimators': 100}),\n",
                "    ('Gradient Boosting', 'gradient_boosting', {'n_estimators': 100}),\n",
                "    ('Logistic Regression', 'logistic_regression', {'max_iter': 1000}),\n",
                "    ('Decision Tree', 'decision_tree', {}),\n",
                "    ('KNN', 'knn', {'n_neighbors': 5}),\n",
                "]\n",
                "\n",
                "# Train each model\n",
                "trained_models = {}\n",
                "\n",
                "for name, model_type, params in model_configs:\n",
                "    print(f\"Training {name}...\")\n",
                "    model = ClassificationModel(model_type, **params)\n",
                "    model.fit(X_train, y_train)\n",
                "    trained_models[name] = model\n",
                "\n",
                "print(\"\\n‚úÖ All models trained!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize comparison\n",
                "comparison = ModelComparison(problem_type='classification')\n",
                "\n",
                "# Add models\n",
                "for name, model in trained_models.items():\n",
                "    comparison.add_model(name, model.underlying_model)\n",
                "\n",
                "# Compare on test set\n",
                "results = comparison.compare(X_test, y_test)\n",
                "\n",
                "print(\"üìä Model Comparison Results:\")\n",
                "display(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get leaderboard sorted by F1 score\n",
                "leaderboard = comparison.get_leaderboard(metric='f1')\n",
                "\n",
                "print(\"üèÜ Leaderboard (by F1 Score):\")\n",
                "display(leaderboard)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Accuracy comparison\n",
                "leaderboard.plot(x='model', y='accuracy', kind='bar', ax=axes[0], color='steelblue', legend=False)\n",
                "axes[0].set_title('Accuracy Comparison')\n",
                "axes[0].set_xlabel('')\n",
                "axes[0].set_ylabel('Accuracy')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# F1 Score comparison\n",
                "leaderboard.plot(x='model', y='f1', kind='bar', ax=axes[1], color='coral', legend=False)\n",
                "axes[1].set_title('F1 Score Comparison')\n",
                "axes[1].set_xlabel('')\n",
                "axes[1].set_ylabel('F1 Score')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Cross-Validation Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-validate models\n",
                "cv_results = comparison.cross_validate(X, y, cv=5)\n",
                "\n",
                "print(\"üìä Cross-Validation Results:\")\n",
                "display(cv_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Auto Model Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use AutoModelSelector to find the best model automatically\n",
                "auto_selector = AutoModelSelector(\n",
                "    problem_type='classification',\n",
                "    cv=5,\n",
                "    models_to_try=['random_forest', 'gradient_boosting', 'logistic_regression']\n",
                ")\n",
                "\n",
                "print(\"üîç Running Auto Model Selection...\")\n",
                "best_model = auto_selector.select(X_train, y_train)\n",
                "\n",
                "print(\"\\nüìä Auto Selection Results:\")\n",
                "display(auto_selector.get_leaderboard())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train and evaluate the best model\n",
                "best_model.fit(X_train, y_train)\n",
                "y_pred = best_model.predict(X_test)\n",
                "\n",
                "metrics = calculate_metrics(y_test, y_pred, 'classification')\n",
                "\n",
                "print(\"üèÜ Best Model Performance:\")\n",
                "for metric, value in metrics.items():\n",
                "    print(f\"  ‚Ä¢ {metric}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Detailed Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_science_master_system.evaluation.metrics import ClassificationMetrics\n",
                "\n",
                "# Get confusion matrix\n",
                "cm = ClassificationMetrics.confusion_matrix(y_test, y_pred)\n",
                "\n",
                "# Plot confusion matrix\n",
                "plotter = Plotter()\n",
                "fig = plotter.confusion_matrix(cm, labels=['No Churn', 'Churn'], title='Confusion Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification report\n",
                "print(\"\\nüìã Classification Report:\")\n",
                "print(ClassificationMetrics.classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "\n",
                "1. **ModelFactory** - Create any model with a unified API\n",
                "2. **ModelComparison** - Compare multiple models easily\n",
                "3. **AutoModelSelector** - Automatically find the best model\n",
                "4. **Cross-validation** - Get robust performance estimates\n",
                "5. **Visualization** - Understand model performance visually\n",
                "\n",
                "### Ready for Intermediate Level! ‚Üí"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}