{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¥ Image Segmentation: U-Net, Mask R-CNN & Detectron2\n",
                "\n",
                "Medical imaging and autonomous driving segmentation.\n",
                "\n",
                "## Learning Outcomes\n",
                "- Semantic vs instance segmentation\n",
                "- U-Net architecture for medical images\n",
                "- Mask R-CNN for instance segmentation\n",
                "- Detectron2 for production deployment\n",
                "\n",
                "**Level**: Advanced | **Time**: 90 min | **GPU**: Recommended"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from torchvision import transforms\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. U-Net Architecture (Medical Imaging)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DoubleConv(nn.Module):\n",
                "    def __init__(self, in_ch, out_ch):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
                "            nn.BatchNorm2d(out_ch),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
                "            nn.BatchNorm2d(out_ch),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        return self.conv(x)\n",
                "\n",
                "class UNet(nn.Module):\n",
                "    def __init__(self, n_channels=3, n_classes=1):\n",
                "        super().__init__()\n",
                "        # Encoder\n",
                "        self.enc1 = DoubleConv(n_channels, 64)\n",
                "        self.enc2 = DoubleConv(64, 128)\n",
                "        self.enc3 = DoubleConv(128, 256)\n",
                "        self.enc4 = DoubleConv(256, 512)\n",
                "        self.pool = nn.MaxPool2d(2)\n",
                "        \n",
                "        # Bottleneck\n",
                "        self.bottleneck = DoubleConv(512, 1024)\n",
                "        \n",
                "        # Decoder\n",
                "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
                "        self.dec4 = DoubleConv(1024, 512)\n",
                "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
                "        self.dec3 = DoubleConv(512, 256)\n",
                "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
                "        self.dec2 = DoubleConv(256, 128)\n",
                "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
                "        self.dec1 = DoubleConv(128, 64)\n",
                "        \n",
                "        self.out = nn.Conv2d(64, n_classes, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Encoder\n",
                "        e1 = self.enc1(x)\n",
                "        e2 = self.enc2(self.pool(e1))\n",
                "        e3 = self.enc3(self.pool(e2))\n",
                "        e4 = self.enc4(self.pool(e3))\n",
                "        \n",
                "        # Bottleneck\n",
                "        b = self.bottleneck(self.pool(e4))\n",
                "        \n",
                "        # Decoder with skip connections\n",
                "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
                "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
                "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
                "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
                "        \n",
                "        return torch.sigmoid(self.out(d1))\n",
                "\n",
                "model = UNet(n_channels=3, n_classes=1).to(device)\n",
                "print(f\"U-Net Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Training U-Net"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DiceLoss(nn.Module):\n",
                "    def __init__(self, smooth=1.):\n",
                "        super().__init__()\n",
                "        self.smooth = smooth\n",
                "    \n",
                "    def forward(self, pred, target):\n",
                "        pred = pred.view(-1)\n",
                "        target = target.view(-1)\n",
                "        intersection = (pred * target).sum()\n",
                "        return 1 - (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
                "\n",
                "def dice_coefficient(pred, target):\n",
                "    pred = (pred > 0.5).float()\n",
                "    intersection = (pred * target).sum()\n",
                "    return (2. * intersection) / (pred.sum() + target.sum() + 1e-8)\n",
                "\n",
                "# Create synthetic data\n",
                "def create_synthetic_data(batch_size=4, size=256):\n",
                "    images = torch.randn(batch_size, 3, size, size)\n",
                "    masks = torch.zeros(batch_size, 1, size, size)\n",
                "    for i in range(batch_size):\n",
                "        cx, cy = np.random.randint(50, size-50, 2)\n",
                "        r = np.random.randint(20, 50)\n",
                "        y, x = np.ogrid[:size, :size]\n",
                "        mask = ((x - cx)**2 + (y - cy)**2 <= r**2).astype(np.float32)\n",
                "        masks[i, 0] = torch.from_numpy(mask)\n",
                "    return images.to(device), masks.to(device)\n",
                "\n",
                "# Training loop\n",
                "criterion = DiceLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
                "\n",
                "print(\"Training U-Net...\")\n",
                "for epoch in range(5):\n",
                "    model.train()\n",
                "    images, masks = create_synthetic_data(8)\n",
                "    optimizer.zero_grad()\n",
                "    pred = model(images)\n",
                "    loss = criterion(pred, masks)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    dice = dice_coefficient(pred, masks)\n",
                "    print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}, Dice={dice.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Mask R-CNN with TorchVision"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
                "\n",
                "# Load pretrained Mask R-CNN\n",
                "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
                "mask_rcnn = maskrcnn_resnet50_fpn(weights=weights).to(device)\n",
                "mask_rcnn.eval()\n",
                "\n",
                "categories = weights.meta['categories']\n",
                "print(f\"Mask R-CNN can segment: {len(categories)} classes\")\n",
                "print(f\"Examples: {categories[:5]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def segment_image(model, image_tensor, threshold=0.5):\n",
                "    \"\"\"Run instance segmentation.\"\"\"\n",
                "    with torch.no_grad():\n",
                "        predictions = model([image_tensor.to(device)])[0]\n",
                "    \n",
                "    mask = predictions['scores'] > threshold\n",
                "    return {\n",
                "        'boxes': predictions['boxes'][mask],\n",
                "        'labels': predictions['labels'][mask],\n",
                "        'masks': predictions['masks'][mask],\n",
                "        'scores': predictions['scores'][mask]\n",
                "    }\n",
                "\n",
                "# Demo with random image\n",
                "test_img = torch.rand(3, 480, 640)\n",
                "result = segment_image(mask_rcnn, test_img)\n",
                "print(f\"Detected {len(result['boxes'])} objects\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Detectron2 (Facebook AI)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from detectron2 import model_zoo\n",
                "    from detectron2.engine import DefaultPredictor\n",
                "    from detectron2.config import get_cfg\n",
                "    \n",
                "    cfg = get_cfg()\n",
                "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
                "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
                "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
                "    \n",
                "    predictor = DefaultPredictor(cfg)\n",
                "    print(\"âœ… Detectron2 loaded successfully\")\n",
                "except ImportError:\n",
                "    print(\"Install Detectron2: pip install detectron2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['U-Net', 'U-Net++', 'Mask R-CNN', 'DeepLab v3+', 'SegFormer'],\n",
                "    'Type': ['Semantic', 'Semantic', 'Instance', 'Semantic', 'Semantic'],\n",
                "    'mIoU (COCO)': ['-', '-', '37.1', '82.1', '84.0'],\n",
                "    'Speed (FPS)': ['50', '40', '5', '10', '15'],\n",
                "    'Best For': ['Medical', 'Medical++', 'Detection+Seg', 'General', 'Transformer']\n",
                "})\n",
                "\n",
                "print(\"ðŸ“Š Segmentation Model Comparison:\")\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Production Deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export U-Net to ONNX\n",
                "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
                "torch.onnx.export(model, dummy_input, 'unet_segmentation.onnx',\n",
                "                  input_names=['image'], output_names=['mask'])\n",
                "print(\"âœ… Model exported to ONNX\")\n",
                "\n",
                "# Cost analysis\n",
                "print(\"\\nðŸ’° Cloud Deployment Costs:\")\n",
                "print(\"  AWS p3.2xlarge (V100): ~$3.06/hour\")\n",
                "print(\"  GCP n1-standard-8 + T4: ~$0.95/hour\")\n",
                "print(\"  Azure NC6 (K80): ~$0.90/hour\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Key Takeaways\n",
                "1. U-Net for medical imaging (skip connections key)\n",
                "2. Mask R-CNN for instance segmentation\n",
                "3. Dice loss for class imbalance\n",
                "4. Detectron2 for production\n",
                "\n",
                "## ðŸ“š Further Reading\n",
                "- Ronneberger et al., \"U-Net\" (2015)\n",
                "- He et al., \"Mask R-CNN\" (2017)\n",
                "- Chen et al., \"DeepLab\" series"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}