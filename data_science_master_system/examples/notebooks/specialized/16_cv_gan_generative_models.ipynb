{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¨ Generative Models: GANs, VAE & StyleGAN\n",
                "\n",
                "Image generation with state-of-the-art generative models.\n",
                "\n",
                "## Learning Outcomes\n",
                "- GAN fundamentals and training\n",
                "- VAE for latent space exploration\n",
                "- StyleGAN for high-quality generation\n",
                "- CycleGAN for image translation\n",
                "\n",
                "**Level**: Advanced | **Time**: 90 min | **GPU**: Required"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Basic GAN Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Generator(nn.Module):\n",
                "    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):\n",
                "        super().__init__()\n",
                "        self.img_shape = img_shape\n",
                "        self.model = nn.Sequential(\n",
                "            nn.Linear(latent_dim, 256),\n",
                "            nn.LeakyReLU(0.2),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Linear(256, 512),\n",
                "            nn.LeakyReLU(0.2),\n",
                "            nn.BatchNorm1d(512),\n",
                "            nn.Linear(512, 1024),\n",
                "            nn.LeakyReLU(0.2),\n",
                "            nn.BatchNorm1d(1024),\n",
                "            nn.Linear(1024, int(np.prod(img_shape))),\n",
                "            nn.Tanh()\n",
                "        )\n",
                "    \n",
                "    def forward(self, z):\n",
                "        img = self.model(z)\n",
                "        return img.view(img.size(0), *self.img_shape)\n",
                "\n",
                "class Discriminator(nn.Module):\n",
                "    def __init__(self, img_shape=(1, 28, 28)):\n",
                "        super().__init__()\n",
                "        self.model = nn.Sequential(\n",
                "            nn.Linear(int(np.prod(img_shape)), 512),\n",
                "            nn.LeakyReLU(0.2),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(512, 256),\n",
                "            nn.LeakyReLU(0.2),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(256, 1),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "    \n",
                "    def forward(self, img):\n",
                "        return self.model(img.view(img.size(0), -1))\n",
                "\n",
                "latent_dim = 100\n",
                "generator = Generator(latent_dim).to(device)\n",
                "discriminator = Discriminator().to(device)\n",
                "print(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. GAN Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load MNIST\n",
                "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
                "mnist = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
                "dataloader = DataLoader(mnist, batch_size=64, shuffle=True)\n",
                "\n",
                "# Optimizers\n",
                "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
                "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
                "criterion = nn.BCELoss()\n",
                "\n",
                "# Training\n",
                "print(\"Training GAN...\")\n",
                "for epoch in range(5):\n",
                "    for i, (real_imgs, _) in enumerate(dataloader):\n",
                "        if i > 100: break  # Quick demo\n",
                "        \n",
                "        real_imgs = real_imgs.to(device)\n",
                "        batch_size = real_imgs.size(0)\n",
                "        \n",
                "        # Labels\n",
                "        real_labels = torch.ones(batch_size, 1).to(device)\n",
                "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
                "        \n",
                "        # Train Discriminator\n",
                "        d_optimizer.zero_grad()\n",
                "        d_real = discriminator(real_imgs)\n",
                "        d_real_loss = criterion(d_real, real_labels)\n",
                "        \n",
                "        z = torch.randn(batch_size, latent_dim).to(device)\n",
                "        fake_imgs = generator(z)\n",
                "        d_fake = discriminator(fake_imgs.detach())\n",
                "        d_fake_loss = criterion(d_fake, fake_labels)\n",
                "        \n",
                "        d_loss = d_real_loss + d_fake_loss\n",
                "        d_loss.backward()\n",
                "        d_optimizer.step()\n",
                "        \n",
                "        # Train Generator\n",
                "        g_optimizer.zero_grad()\n",
                "        g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
                "        g_loss.backward()\n",
                "        g_optimizer.step()\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}: D_loss={d_loss.item():.4f}, G_loss={g_loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Variational Autoencoder (VAE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VAE(nn.Module):\n",
                "    def __init__(self, latent_dim=20):\n",
                "        super().__init__()\n",
                "        self.latent_dim = latent_dim\n",
                "        \n",
                "        # Encoder\n",
                "        self.encoder = nn.Sequential(\n",
                "            nn.Linear(784, 400),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        self.fc_mu = nn.Linear(400, latent_dim)\n",
                "        self.fc_var = nn.Linear(400, latent_dim)\n",
                "        \n",
                "        # Decoder\n",
                "        self.decoder = nn.Sequential(\n",
                "            nn.Linear(latent_dim, 400),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(400, 784),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "    \n",
                "    def encode(self, x):\n",
                "        h = self.encoder(x.view(-1, 784))\n",
                "        return self.fc_mu(h), self.fc_var(h)\n",
                "    \n",
                "    def reparameterize(self, mu, logvar):\n",
                "        std = torch.exp(0.5 * logvar)\n",
                "        eps = torch.randn_like(std)\n",
                "        return mu + eps * std\n",
                "    \n",
                "    def decode(self, z):\n",
                "        return self.decoder(z)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        mu, logvar = self.encode(x)\n",
                "        z = self.reparameterize(mu, logvar)\n",
                "        return self.decode(z), mu, logvar\n",
                "\n",
                "def vae_loss(recon_x, x, mu, logvar):\n",
                "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
                "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
                "    return BCE + KLD\n",
                "\n",
                "vae = VAE(latent_dim=20).to(device)\n",
                "print(f\"VAE params: {sum(p.numel() for p in vae.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. StyleGAN (Pretrained)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using pretrained StyleGAN from torch hub\n",
                "try:\n",
                "    # Note: StyleGAN requires specific setup\n",
                "    print(\"StyleGAN2 key concepts:\")\n",
                "    print(\"  - Progressive growing for stability\")\n",
                "    print(\"  - Style-based synthesis\")\n",
                "    print(\"  - Adaptive instance normalization (AdaIN)\")\n",
                "    print(\"  - Latent space manipulation for editing\")\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# Latent space interpolation demo\n",
                "def interpolate_latent(generator, z1, z2, steps=10):\n",
                "    \"\"\"Interpolate between two latent vectors.\"\"\"\n",
                "    images = []\n",
                "    for alpha in np.linspace(0, 1, steps):\n",
                "        z = (1 - alpha) * z1 + alpha * z2\n",
                "        with torch.no_grad():\n",
                "            img = generator(z)\n",
                "        images.append(img)\n",
                "    return images\n",
                "\n",
                "# Demo interpolation\n",
                "z1 = torch.randn(1, latent_dim).to(device)\n",
                "z2 = torch.randn(1, latent_dim).to(device)\n",
                "interp_imgs = interpolate_latent(generator, z1, z2, 5)\n",
                "print(f\"Generated {len(interp_imgs)} interpolated images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['GAN', 'DCGAN', 'VAE', 'StyleGAN2', 'Diffusion'],\n",
                "    'Quality (FIDâ†“)': [50, 30, 70, 3, 2],\n",
                "    'Training': ['Hard', 'Medium', 'Easy', 'Hard', 'Medium'],\n",
                "    'Latent Control': ['Low', 'Medium', 'High', 'Very High', 'Medium'],\n",
                "    'Best For': ['Basic', 'Images', 'Latent Space', 'Faces', 'Quality']\n",
                "})\n",
                "\n",
                "print(\"ðŸ“Š Generative Model Comparison:\")\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Applications"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "applications = [\n",
                "    \"ðŸŽ¨ Art Generation - Create unique artwork\",\n",
                "    \"ðŸ“¸ Data Augmentation - Generate training data\",\n",
                "    \"ðŸ‘¤ Face Generation - Synthetic avatars\",\n",
                "    \"ðŸ”„ Image-to-Image - Style transfer\",\n",
                "    \"ðŸŽ® Game Assets - Procedural content\",\n",
                "    \"ðŸ‘— Fashion - Virtual try-on\"\n",
                "]\n",
                "\n",
                "print(\"ðŸš€ GAN Applications:\")\n",
                "for app in applications:\n",
                "    print(f\"  {app}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Key Takeaways\n",
                "1. GANs: adversarial training (generator vs discriminator)\n",
                "2. VAE: smooth latent space, explicit distribution\n",
                "3. StyleGAN: style-based control, high quality\n",
                "4. Diffusion models: new SOTA for quality\n",
                "\n",
                "## ðŸ“š Further Reading\n",
                "- Goodfellow et al., \"Generative Adversarial Networks\" (2014)\n",
                "- Kingma & Welling, \"Auto-Encoding Variational Bayes\" (2013)\n",
                "- Karras et al., \"A Style-Based Generator Architecture\" (2019)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}