{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚úçÔ∏è Text Generation: GPT, T5 & LLMs\n",
                "\n",
                "Modern text generation with transformer-based language models.\n",
                "\n",
                "## Learning Outcomes\n",
                "- GPT-style autoregressive generation\n",
                "- T5 for text-to-text tasks\n",
                "- Controlled text generation\n",
                "- Practical applications\n",
                "\n",
                "**Level**: Advanced | **Time**: 75 min | **GPU**: Recommended"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
                "from transformers import pipeline\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. GPT-2 Text Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load GPT-2\n",
                "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
                "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
                "gpt2_model.eval()\n",
                "\n",
                "print(f\"GPT-2 Parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
                "    \"\"\"Generate text with GPT-2.\"\"\"\n",
                "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            inputs,\n",
                "            max_length=max_length,\n",
                "            temperature=temperature,\n",
                "            top_p=top_p,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Generate text\n",
                "prompt = \"The future of artificial intelligence is\"\n",
                "generated = generate_text(gpt2_model, gpt2_tokenizer, prompt)\n",
                "print(f\"\\nüìù Generated Text:\\n{generated}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Decoding Strategies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_decoding(prompt):\n",
                "    \"\"\"Compare different decoding strategies.\"\"\"\n",
                "    inputs = gpt2_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
                "    \n",
                "    strategies = {\n",
                "        'Greedy': {'do_sample': False},\n",
                "        'Temperature (0.5)': {'do_sample': True, 'temperature': 0.5},\n",
                "        'Temperature (1.5)': {'do_sample': True, 'temperature': 1.5},\n",
                "        'Top-p (0.9)': {'do_sample': True, 'top_p': 0.9},\n",
                "        'Top-k (50)': {'do_sample': True, 'top_k': 50}\n",
                "    }\n",
                "    \n",
                "    print(f\"Prompt: '{prompt}'\\n\")\n",
                "    for name, params in strategies.items():\n",
                "        with torch.no_grad():\n",
                "            output = gpt2_model.generate(\n",
                "                inputs, max_length=50, pad_token_id=gpt2_tokenizer.eos_token_id, **params\n",
                "            )\n",
                "        text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
                "        print(f\"{name}: {text[:100]}...\\n\")\n",
                "\n",
                "compare_decoding(\"Machine learning is\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. T5 for Text-to-Text Tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load T5\n",
                "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
                "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
                "t5_model.eval()\n",
                "\n",
                "print(f\"T5-small Parameters: {sum(p.numel() for p in t5_model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def t5_generate(task_prefix, input_text, max_length=100):\n",
                "    \"\"\"Generate with T5 using task prefix.\"\"\"\n",
                "    input_ids = t5_tokenizer.encode(f\"{task_prefix}: {input_text}\", return_tensors='pt').to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = t5_model.generate(input_ids, max_length=max_length)\n",
                "    \n",
                "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Summarization\n",
                "article = \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It uses algorithms to find patterns in data and make predictions.\"\n",
                "summary = t5_generate(\"summarize\", article)\n",
                "print(f\"üìÑ Summary: {summary}\")\n",
                "\n",
                "# Translation\n",
                "english = \"Hello, how are you today?\"\n",
                "german = t5_generate(\"translate English to German\", english)\n",
                "print(f\"üá©üá™ German: {german}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Controlled Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_with_keywords(prompt, keywords, max_length=100):\n",
                "    \"\"\"Guide generation with keywords.\"\"\"\n",
                "    keyword_str = ', '.join(keywords)\n",
                "    full_prompt = f\"{prompt} (keywords: {keyword_str})\"\n",
                "    return generate_text(gpt2_model, gpt2_tokenizer, full_prompt, max_length)\n",
                "\n",
                "# Generate with topic guidance\n",
                "result = generate_with_keywords(\n",
                "    \"Write about technology:\",\n",
                "    ['innovation', 'future', 'AI']\n",
                ")\n",
                "print(f\"üìù Controlled generation:\\n{result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Code Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using code-specific model\n",
                "try:\n",
                "    code_generator = pipeline('text-generation', model='Salesforce/codegen-350M-mono', device=0 if torch.cuda.is_available() else -1)\n",
                "    \n",
                "    code_prompt = \"def calculate_fibonacci(n):\"\n",
                "    generated_code = code_generator(code_prompt, max_length=100)[0]['generated_text']\n",
                "    print(f\"üíª Generated Code:\\n{generated_code}\")\n",
                "except:\n",
                "    print(\"For code generation, try: codegen, starcoder, or codellama\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['GPT-2', 'GPT-3', 'T5', 'FLAN-T5', 'LLaMA 2', 'Claude'],\n",
                "    'Parameters': ['117M-1.5B', '175B', '60M-11B', '80M-11B', '7B-70B', '?'],\n",
                "    'Type': ['Decoder', 'Decoder', 'Enc-Dec', 'Enc-Dec', 'Decoder', 'Decoder'],\n",
                "    'Open': ['Yes', 'No', 'Yes', 'Yes', 'Yes', 'No'],\n",
                "    'Best For': ['General', 'Complex', 'Multitask', 'Instructions', 'Open-source', 'Safety']\n",
                "})\n",
                "\n",
                "print(\"üìä LLM Comparison:\")\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Production Deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Deployment Options:\")\n",
                "print(\"  1. HuggingFace Inference API - Easy, pay-per-use\")\n",
                "print(\"  2. vLLM - Fast local inference with PagedAttention\")\n",
                "print(\"  3. TensorRT-LLM - NVIDIA optimized\")\n",
                "print(\"  4. GGML/llama.cpp - CPU inference\")\n",
                "print(\"  5. Ollama - Simple local deployment\")\n",
                "\n",
                "print(\"\\nüí∞ Cost Analysis (1M tokens/day):\")\n",
                "print(\"  OpenAI GPT-4: ~$60/day\")\n",
                "print(\"  OpenAI GPT-3.5: ~$2/day\")\n",
                "print(\"  Self-hosted LLaMA 7B: ~$50/month (GPU)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "1. Temperature controls randomness\n",
                "2. Top-p (nucleus) for quality\n",
                "3. T5 task prefixes for flexibility\n",
                "4. Quantization for deployment\n",
                "\n",
                "## üìö Further Reading\n",
                "- Radford et al., \"Language Models are Unsupervised Multitask Learners\" (GPT-2)\n",
                "- Raffel et al., \"Exploring Limits of Transfer Learning with T5\"\n",
                "- Touvron et al., \"LLaMA: Open Foundation Models\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}