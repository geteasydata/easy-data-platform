{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¤” Question Answering: BERT & T5 for QA\n",
                "\n",
                "Build extractive and generative QA systems.\n",
                "\n",
                "## Learning Outcomes\n",
                "- Extractive QA with BERT/RoBERTa\n",
                "- Generative QA with T5/FLAN-T5\n",
                "- Multi-document QA\n",
                "- Production deployment\n",
                "\n",
                "**Level**: Advanced | **Time**: 60 min | **GPU**: Recommended"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoModelForQuestionAnswering, AutoTokenizer,\n",
                "    T5ForConditionalGeneration, T5Tokenizer,\n",
                "    pipeline\n",
                ")\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Extractive QA with BERT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load pretrained QA model\n",
                "qa_model_name = 'distilbert-base-cased-distilled-squad'\n",
                "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
                "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(device)\n",
                "qa_model.eval()\n",
                "\n",
                "print(f\"Model: {qa_model_name}\")\n",
                "print(f\"Parameters: {sum(p.numel() for p in qa_model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_answer(question, context, model, tokenizer):\n",
                "    \"\"\"Extract answer span from context.\"\"\"\n",
                "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    \n",
                "    start_idx = outputs.start_logits.argmax()\n",
                "    end_idx = outputs.end_logits.argmax()\n",
                "    \n",
                "    tokens = inputs.input_ids[0][start_idx:end_idx+1]\n",
                "    answer = tokenizer.decode(tokens, skip_special_tokens=True)\n",
                "    \n",
                "    confidence = (outputs.start_logits[0][start_idx] + outputs.end_logits[0][end_idx]).item() / 2\n",
                "    \n",
                "    return answer, confidence\n",
                "\n",
                "# Example\n",
                "context = \"\"\"Machine learning is a subset of artificial intelligence that enables computers \n",
                "to learn from data without being explicitly programmed. It was founded by Arthur Samuel in 1959. \n",
                "Today, ML is used in recommendation systems, autonomous vehicles, and medical diagnosis.\"\"\"\n",
                "\n",
                "questions = [\n",
                "    \"What is machine learning?\",\n",
                "    \"Who founded machine learning?\",\n",
                "    \"What is ML used for today?\"\n",
                "]\n",
                "\n",
                "print(\"ðŸ“š Context:\", context[:100], \"...\\n\")\n",
                "for q in questions:\n",
                "    answer, conf = extract_answer(q, context, qa_model, qa_tokenizer)\n",
                "    print(f\"Q: {q}\")\n",
                "    print(f\"A: {answer} (confidence: {conf:.2f})\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Using Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple pipeline approach\n",
                "qa_pipeline = pipeline('question-answering', model=qa_model_name, device=0 if torch.cuda.is_available() else -1)\n",
                "\n",
                "result = qa_pipeline(question=\"When was ML founded?\", context=context)\n",
                "print(f\"Answer: {result['answer']}\")\n",
                "print(f\"Score: {result['score']:.4f}\")\n",
                "print(f\"Span: {result['start']} - {result['end']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generative QA with T5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load T5 for generative QA\n",
                "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
                "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
                "t5_model.eval()\n",
                "\n",
                "def generative_qa(question, context, max_length=100):\n",
                "    \"\"\"Generate answer using T5.\"\"\"\n",
                "    input_text = f\"question: {question} context: {context}\"\n",
                "    inputs = t5_tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = t5_model.generate(**inputs, max_length=max_length)\n",
                "    \n",
                "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test generative QA\n",
                "answer = generative_qa(\"Explain what machine learning does\", context)\n",
                "print(f\"ðŸ¤– Generative Answer: {answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Multi-Document QA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = [\n",
                "    \"Python is a high-level programming language created by Guido van Rossum in 1991.\",\n",
                "    \"JavaScript was created by Brendan Eich in 1995 for web browsers.\",\n",
                "    \"Java was developed by James Gosling at Sun Microsystems in 1995.\"\n",
                "]\n",
                "\n",
                "def multi_doc_qa(question, documents, top_k=1):\n",
                "    \"\"\"Answer question from multiple documents.\"\"\"\n",
                "    results = []\n",
                "    for doc in documents:\n",
                "        answer, score = extract_answer(question, doc, qa_model, qa_tokenizer)\n",
                "        if answer.strip():\n",
                "            results.append({'answer': answer, 'score': score, 'source': doc[:50]})\n",
                "    \n",
                "    results.sort(key=lambda x: x['score'], reverse=True)\n",
                "    return results[:top_k]\n",
                "\n",
                "# Test multi-doc QA\n",
                "question = \"Who created Python?\"\n",
                "results = multi_doc_qa(question, documents)\n",
                "print(f\"Q: {question}\")\n",
                "for r in results:\n",
                "    print(f\"A: {r['answer']} (from: {r['source']}...)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['BERT-base', 'DistilBERT', 'RoBERTa', 'T5', 'FLAN-T5'],\n",
                "    'Type': ['Extractive', 'Extractive', 'Extractive', 'Generative', 'Generative'],\n",
                "    'SQuAD F1': ['88.5', '86.9', '91.5', '89.0', '91.2'],\n",
                "    'Speed': ['1x', '2x', '0.9x', '0.5x', '0.5x'],\n",
                "    'Best For': ['General', 'Fast', 'Accuracy', 'Complex', 'Instructions']\n",
                "})\n",
                "\n",
                "print(\"ðŸ“Š QA Model Comparison:\")\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Production QA System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QASystem:\n",
                "    \"\"\"Production-ready QA system.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name='distilbert-base-cased-distilled-squad'):\n",
                "        self.pipeline = pipeline('question-answering', model=model_name)\n",
                "        self.knowledge_base = []\n",
                "    \n",
                "    def add_documents(self, documents):\n",
                "        self.knowledge_base.extend(documents)\n",
                "    \n",
                "    def answer(self, question, threshold=0.1):\n",
                "        best_answer = None\n",
                "        best_score = 0\n",
                "        \n",
                "        for doc in self.knowledge_base:\n",
                "            result = self.pipeline(question=question, context=doc)\n",
                "            if result['score'] > best_score:\n",
                "                best_score = result['score']\n",
                "                best_answer = result['answer']\n",
                "        \n",
                "        if best_score < threshold:\n",
                "            return \"I don't have enough information to answer that.\"\n",
                "        \n",
                "        return best_answer\n",
                "\n",
                "# Demo\n",
                "qa_system = QASystem()\n",
                "qa_system.add_documents(documents)\n",
                "print(f\"Answer: {qa_system.answer('Who created JavaScript?')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Key Takeaways\n",
                "1. Extractive QA finds answer spans\n",
                "2. Generative QA synthesizes answers\n",
                "3. Multi-doc QA for knowledge bases\n",
                "4. Confidence thresholds prevent hallucination\n",
                "\n",
                "## ðŸ“š Further Reading\n",
                "- Devlin et al., \"BERT\" (2019)\n",
                "- Rajpurkar et al., \"SQuAD\" dataset\n",
                "- Lewis et al., \"RAG\" for retrieval-augmented QA"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}