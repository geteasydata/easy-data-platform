{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ® Reinforcement Learning: Q-Learning to PPO\n",
                "\n",
                "From basic RL to deep reinforcement learning.\n",
                "\n",
                "## Learning Outcomes\n",
                "- Q-learning fundamentals\n",
                "- Deep Q-Networks (DQN)\n",
                "- Policy gradient methods (PPO)\n",
                "- Practical applications\n",
                "\n",
                "**Level**: Advanced | **Time**: 90 min | **GPU**: Optional"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import deque\n",
                "import random\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Simple Environment (GridWorld)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GridWorld:\n",
                "    \"\"\"Simple 4x4 GridWorld environment.\"\"\"\n",
                "    def __init__(self, size=4):\n",
                "        self.size = size\n",
                "        self.goal = (size-1, size-1)\n",
                "        self.state = None\n",
                "        self.reset()\n",
                "    \n",
                "    def reset(self):\n",
                "        self.state = (0, 0)\n",
                "        return self._state_to_idx()\n",
                "    \n",
                "    def _state_to_idx(self):\n",
                "        return self.state[0] * self.size + self.state[1]\n",
                "    \n",
                "    def step(self, action):\n",
                "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
                "        moves = [(-1,0), (0,1), (1,0), (0,-1)]\n",
                "        new_row = max(0, min(self.size-1, self.state[0] + moves[action][0]))\n",
                "        new_col = max(0, min(self.size-1, self.state[1] + moves[action][1]))\n",
                "        self.state = (new_row, new_col)\n",
                "        \n",
                "        done = self.state == self.goal\n",
                "        reward = 1.0 if done else -0.01\n",
                "        \n",
                "        return self._state_to_idx(), reward, done\n",
                "\n",
                "env = GridWorld(4)\n",
                "print(f\"Grid: {env.size}x{env.size}, Goal: {env.goal}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Q-Learning (Tabular)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QLearningAgent:\n",
                "    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.99, epsilon=1.0):\n",
                "        self.q_table = np.zeros((n_states, n_actions))\n",
                "        self.lr = lr\n",
                "        self.gamma = gamma\n",
                "        self.epsilon = epsilon\n",
                "        self.epsilon_min = 0.01\n",
                "        self.epsilon_decay = 0.995\n",
                "        self.n_actions = n_actions\n",
                "    \n",
                "    def get_action(self, state):\n",
                "        if np.random.random() < self.epsilon:\n",
                "            return np.random.randint(self.n_actions)\n",
                "        return np.argmax(self.q_table[state])\n",
                "    \n",
                "    def update(self, state, action, reward, next_state, done):\n",
                "        best_next = np.max(self.q_table[next_state]) if not done else 0\n",
                "        target = reward + self.gamma * best_next\n",
                "        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])\n",
                "        \n",
                "        if done:\n",
                "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
                "\n",
                "# Train Q-learning\n",
                "agent = QLearningAgent(n_states=16, n_actions=4)\n",
                "rewards_history = []\n",
                "\n",
                "print(\"Training Q-Learning...\")\n",
                "for episode in range(500):\n",
                "    state = env.reset()\n",
                "    total_reward = 0\n",
                "    \n",
                "    for _ in range(100):\n",
                "        action = agent.get_action(state)\n",
                "        next_state, reward, done = env.step(action)\n",
                "        agent.update(state, action, reward, next_state, done)\n",
                "        total_reward += reward\n",
                "        state = next_state\n",
                "        if done:\n",
                "            break\n",
                "    \n",
                "    rewards_history.append(total_reward)\n",
                "    \n",
                "    if (episode + 1) % 100 == 0:\n",
                "        avg_reward = np.mean(rewards_history[-100:])\n",
                "        print(f\"Episode {episode+1}: Avg Reward={avg_reward:.3f}, Epsilon={agent.epsilon:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Deep Q-Network (DQN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DQN(nn.Module):\n",
                "    def __init__(self, n_states, n_actions):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(n_states, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, n_actions)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class ReplayBuffer:\n",
                "    def __init__(self, capacity=10000):\n",
                "        self.buffer = deque(maxlen=capacity)\n",
                "    \n",
                "    def push(self, state, action, reward, next_state, done):\n",
                "        self.buffer.append((state, action, reward, next_state, done))\n",
                "    \n",
                "    def sample(self, batch_size):\n",
                "        batch = random.sample(self.buffer, batch_size)\n",
                "        return zip(*batch)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.buffer)\n",
                "\n",
                "class DQNAgent:\n",
                "    def __init__(self, n_states, n_actions):\n",
                "        self.n_actions = n_actions\n",
                "        self.policy_net = DQN(n_states, n_actions).to(device)\n",
                "        self.target_net = DQN(n_states, n_actions).to(device)\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n",
                "        self.buffer = ReplayBuffer()\n",
                "        self.gamma = 0.99\n",
                "        self.epsilon = 1.0\n",
                "    \n",
                "    def get_action(self, state):\n",
                "        if random.random() < self.epsilon:\n",
                "            return random.randint(0, self.n_actions - 1)\n",
                "        with torch.no_grad():\n",
                "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "            return self.policy_net(state_t).argmax().item()\n",
                "    \n",
                "    def update(self, batch_size=32):\n",
                "        if len(self.buffer) < batch_size:\n",
                "            return\n",
                "        \n",
                "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
                "        \n",
                "        states = torch.FloatTensor(states).to(device)\n",
                "        actions = torch.LongTensor(actions).to(device)\n",
                "        rewards = torch.FloatTensor(rewards).to(device)\n",
                "        next_states = torch.FloatTensor(next_states).to(device)\n",
                "        dones = torch.FloatTensor(dones).to(device)\n",
                "        \n",
                "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
                "        next_q = self.target_net(next_states).max(1)[0].detach()\n",
                "        target_q = rewards + self.gamma * next_q * (1 - dones)\n",
                "        \n",
                "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "\n",
                "print(f\"DQN Parameters: {sum(p.numel() for p in DQN(16, 4).parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Policy Gradient (REINFORCE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PolicyNetwork(nn.Module):\n",
                "    def __init__(self, n_states, n_actions):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(n_states, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, n_actions),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class REINFORCEAgent:\n",
                "    def __init__(self, n_states, n_actions, lr=1e-3):\n",
                "        self.policy = PolicyNetwork(n_states, n_actions).to(device)\n",
                "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
                "        self.gamma = 0.99\n",
                "        self.saved_log_probs = []\n",
                "        self.rewards = []\n",
                "    \n",
                "    def get_action(self, state):\n",
                "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        probs = self.policy(state_t)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "        self.saved_log_probs.append(dist.log_prob(action))\n",
                "        return action.item()\n",
                "    \n",
                "    def update(self):\n",
                "        R = 0\n",
                "        returns = []\n",
                "        for r in reversed(self.rewards):\n",
                "            R = r + self.gamma * R\n",
                "            returns.insert(0, R)\n",
                "        \n",
                "        returns = torch.tensor(returns).to(device)\n",
                "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
                "        \n",
                "        loss = []\n",
                "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
                "            loss.append(-log_prob * R)\n",
                "        \n",
                "        self.optimizer.zero_grad()\n",
                "        torch.stack(loss).sum().backward()\n",
                "        self.optimizer.step()\n",
                "        \n",
                "        self.saved_log_probs = []\n",
                "        self.rewards = []\n",
                "\n",
                "print(\"REINFORCE agent ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. PPO Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PPO is more complex - here's the key idea\n",
                "print(\"ðŸŽ¯ PPO Key Concepts:\")\n",
                "print(\"  1. Actor-Critic: Value + Policy networks\")\n",
                "print(\"  2. Clipped objective: Prevents large updates\")\n",
                "print(\"  3. Multiple epochs: Better sample efficiency\")\n",
                "print(\"  4. GAE: Generalized Advantage Estimation\")\n",
                "print(\"\\nðŸ“¦ Libraries:\")\n",
                "print(\"  - stable-baselines3: PPO, A2C, SAC\")\n",
                "print(\"  - cleanrl: Simple reference implementations\")\n",
                "print(\"  - rllib: Distributed RL\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Algorithm': ['Q-Learning', 'DQN', 'REINFORCE', 'A2C', 'PPO', 'SAC'],\n",
                "    'Type': ['Value', 'Value', 'Policy', 'Actor-Critic', 'Actor-Critic', 'Actor-Critic'],\n",
                "    'Actions': ['Discrete', 'Discrete', 'Both', 'Both', 'Both', 'Continuous'],\n",
                "    'Sample Eff.': ['Low', 'Medium', 'Low', 'Medium', 'High', 'High'],\n",
                "    'Best For': ['Simple', 'Atari', 'Simple PG', 'Fast', 'General', 'Robotics']\n",
                "})\n",
                "\n",
                "print(\"ðŸ“Š RL Algorithm Comparison:\")\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Applications"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "applications = [\n",
                "    (\"ðŸŽ® Games\", \"AlphaGo, Atari, StarCraft\"),\n",
                "    (\"ðŸ¤– Robotics\", \"Manipulation, locomotion\"),\n",
                "    (\"ðŸ’° Trading\", \"Portfolio optimization\"),\n",
                "    (\"ðŸš— Autonomous\", \"Self-driving decisions\"),\n",
                "    (\"ðŸ“± RecSys\", \"Content sequencing\"),\n",
                "    (\"ðŸ­ Industry\", \"Process control\"),\n",
                "]\n",
                "\n",
                "print(\"ðŸš€ RL Applications:\")\n",
                "for name, desc in applications:\n",
                "    print(f\"  {name}: {desc}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Key Takeaways\n",
                "1. Q-learning: value-based, discrete\n",
                "2. DQN: deep learning + experience replay\n",
                "3. Policy gradients: directly optimize policy\n",
                "4. PPO: stable, general-purpose\n",
                "\n",
                "## ðŸ“š Further Reading\n",
                "- Sutton & Barto, \"Reinforcement Learning: An Introduction\"\n",
                "- Mnih et al., \"Human-level control through deep RL\" (DQN)\n",
                "- Schulman et al., \"Proximal Policy Optimization\" (PPO)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}