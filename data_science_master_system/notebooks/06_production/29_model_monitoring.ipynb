{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì° Model Monitoring & Alerting\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê Advanced  \n",
                "**Time**: 45 minutes  \n",
                "**Prerequisites**: 28_cloud_deployment\n",
                "\n",
                "## Learning Objectives\n",
                "- Data drift detection\n",
                "- Model performance monitoring\n",
                "- Alert systems\n",
                "- Observability dashboards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Drift Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DriftDetector:\n",
                "    def __init__(self, reference_data, threshold=0.05):\n",
                "        self.reference = reference_data\n",
                "        self.threshold = threshold\n",
                "    \n",
                "    def detect_drift(self, new_data, feature):\n",
                "        \"\"\"KS test for drift.\"\"\"\n",
                "        stat, p_value = stats.ks_2samp(self.reference[feature], new_data[feature])\n",
                "        return {\n",
                "            'feature': feature,\n",
                "            'drift_detected': p_value < self.threshold,\n",
                "            'p_value': p_value,\n",
                "            'statistic': stat\n",
                "        }\n",
                "    \n",
                "    def check_all(self, new_data):\n",
                "        results = []\n",
                "        for col in self.reference.columns:\n",
                "            if col in new_data.columns and np.issubdtype(new_data[col].dtype, np.number):\n",
                "                results.append(self.detect_drift(new_data, col))\n",
                "        return pd.DataFrame(results)\n",
                "\n",
                "# Demo\n",
                "reference = pd.DataFrame({'age': np.random.normal(35, 10, 1000), 'income': np.random.normal(50000, 15000, 1000)})\n",
                "new_data = pd.DataFrame({'age': np.random.normal(40, 10, 500), 'income': np.random.normal(55000, 15000, 500)})  # Drifted!\n",
                "\n",
                "detector = DriftDetector(reference)\n",
                "drift_results = detector.check_all(new_data)\n",
                "print(\"üìä Drift Detection:\")\n",
                "display(drift_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Performance Monitoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PerformanceMonitor:\n",
                "    def __init__(self, window_size=100):\n",
                "        self.predictions = []\n",
                "        self.actuals = []\n",
                "        self.window = window_size\n",
                "    \n",
                "    def log(self, prediction, actual):\n",
                "        self.predictions.append(prediction)\n",
                "        self.actuals.append(actual)\n",
                "    \n",
                "    def get_metrics(self):\n",
                "        recent_pred = self.predictions[-self.window:]\n",
                "        recent_actual = self.actuals[-self.window:]\n",
                "        \n",
                "        accuracy = sum(p == a for p, a in zip(recent_pred, recent_actual)) / len(recent_pred)\n",
                "        return {\n",
                "            'accuracy': accuracy,\n",
                "            'sample_size': len(recent_pred),\n",
                "            'timestamp': datetime.now().isoformat()\n",
                "        }\n",
                "\n",
                "# Demo\n",
                "monitor = PerformanceMonitor()\n",
                "for _ in range(150):\n",
                "    pred = np.random.choice([0, 1])\n",
                "    actual = np.random.choice([0, 1], p=[0.3, 0.7] if pred == 1 else [0.7, 0.3])\n",
                "    monitor.log(pred, actual)\n",
                "\n",
                "print(\"üìà Performance Metrics:\")\n",
                "print(monitor.get_metrics())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Alert System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AlertManager:\n",
                "    def __init__(self):\n",
                "        self.alerts = []\n",
                "        self.thresholds = {\n",
                "            'accuracy': 0.85,\n",
                "            'latency_ms': 200,\n",
                "            'error_rate': 0.05\n",
                "        }\n",
                "    \n",
                "    def check(self, metric, value):\n",
                "        threshold = self.thresholds.get(metric)\n",
                "        if threshold:\n",
                "            if metric == 'accuracy' and value < threshold:\n",
                "                self._trigger(f\"‚ö†Ô∏è Low accuracy: {value:.2%} < {threshold:.2%}\")\n",
                "            elif metric in ['latency_ms', 'error_rate'] and value > threshold:\n",
                "                self._trigger(f\"üö® High {metric}: {value} > {threshold}\")\n",
                "    \n",
                "    def _trigger(self, message):\n",
                "        alert = {'message': message, 'time': datetime.now().isoformat()}\n",
                "        self.alerts.append(alert)\n",
                "        print(f\"ALERT: {message}\")\n",
                "        # In production: send Slack/email/PagerDuty\n",
                "\n",
                "# Demo\n",
                "alerts = AlertManager()\n",
                "alerts.check('accuracy', 0.78)\n",
                "alerts.check('latency_ms', 250)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prometheus Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prometheus_config = '''\n",
                "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
                "\n",
                "# Define metrics\n",
                "predictions = Counter('model_predictions_total', 'Total predictions', ['class'])\n",
                "latency = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
                "accuracy = Gauge('model_accuracy', 'Current model accuracy')\n",
                "\n",
                "# Use in API\n",
                "@app.post(\"/predict\")\n",
                "@latency.time()\n",
                "def predict(data):\n",
                "    result = model.predict(data)\n",
                "    predictions.labels(class=str(result)).inc()\n",
                "    return result\n",
                "\n",
                "# Export metrics\n",
                "start_http_server(9090)  # Prometheus scrapes this\n",
                "'''\n",
                "print(\"üìä Prometheus Integration:\")\n",
                "print(prometheus_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "- Drift detection prevents silent failures\n",
                "- Real-time monitoring catches issues\n",
                "- Alerts enable fast response\n",
                "- Dashboards provide visibility\n",
                "\n",
                "**üéâ Congratulations!** You've completed the entire curriculum!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}