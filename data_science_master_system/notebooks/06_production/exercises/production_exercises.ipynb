{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ MLOps & Production - Exercises\n",
                "\n",
                "Practice deployment, monitoring, and production ML."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 1: Fill-in-the-Blank (‚≠ê)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1.1: Create FastAPI endpoint\n",
                "from ________ import FastAPI\n",
                "from pydantic import BaseModel\n",
                "\n",
                "app = ________()\n",
                "\n",
                "class PredictRequest(BaseModel):\n",
                "    features: list\n",
                "\n",
                "@app.________(________)  # POST /predict\n",
                "def predict(request: PredictRequest):\n",
                "    return {\"prediction\": [0, 1]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1.2: Save model with MLflow\n",
                "import ________\n",
                "\n",
                "with mlflow.________():\n",
                "    mlflow.log_______('accuracy', 0.95)\n",
                "    mlflow.sklearn.log_______('model', model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 2: Code Completion (‚≠ê‚≠ê)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2.1: Implement model serving API\n",
                "# Complete the prediction endpoint with error handling\n",
                "\n",
                "from fastapi import FastAPI, HTTPException\n",
                "import joblib\n",
                "import numpy as np\n",
                "\n",
                "app = FastAPI()\n",
                "model = joblib.load('model.joblib')\n",
                "\n",
                "@app.post(\"/predict\")\n",
                "def predict(data: dict):\n",
                "    try:\n",
                "        # 1. Extract features from data\n",
                "        features = # YOUR CODE\n",
                "        \n",
                "        # 2. Validate input shape\n",
                "        # YOUR CODE\n",
                "        \n",
                "        # 3. Make prediction\n",
                "        prediction = # YOUR CODE\n",
                "        \n",
                "        # 4. Return response\n",
                "        return # YOUR CODE\n",
                "        \n",
                "    except Exception as e:\n",
                "        raise HTTPException(status_code=500, detail=str(e))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 3: Debug (‚≠ê‚≠ê‚≠ê)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3.1: Fix the Dockerfile (3 bugs)\n",
                "\n",
                "dockerfile = '''\n",
                "FROM python:latest  # Bug 1: Too large, use slim\n",
                "\n",
                "WORKDIR /app\n",
                "\n",
                "COPY . .  # Bug 2: Should copy requirements first for caching\n",
                "RUN pip install -r requirements.txt\n",
                "\n",
                "# Bug 3: Missing EXPOSE for port\n",
                "CMD [\"python\", \"app.py\"]  # Bug 4: Should use proper WSGI server\n",
                "'''\n",
                "\n",
                "# Write the corrected Dockerfile"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 4: Business Case (‚≠ê‚≠ê‚≠ê‚≠ê)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CHALLENGE: Production ML System Design\n",
                "#\n",
                "# Scenario: Design a recommendation system for a streaming platform\n",
                "#\n",
                "# Requirements:\n",
                "# 1. Serve 10M users, 100K requests/second\n",
                "# 2. Latency < 50ms p99\n",
                "# 3. Update recommendations daily\n",
                "# 4. A/B testing for new models\n",
                "# 5. Monitor for drift and performance\n",
                "#\n",
                "# Design:\n",
                "# - Architecture diagram\n",
                "# - Technology choices (with justification)\n",
                "# - Cost estimate (monthly)\n",
                "# - Monitoring strategy\n",
                "\n",
                "# YOUR DESIGN DOCUMENT"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Quiz\n",
                "\n",
                "**Q1**: What's the difference between batch and real-time inference?\n",
                "\n",
                "**Q2**: Why use Docker for ML deployment?\n",
                "\n",
                "**Q3**: What is model drift and how do you detect it?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üîë Solutions\n",
                "<details>\n",
                "<summary>Click to reveal</summary>\n",
                "\n",
                "**1.1**: fastapi, FastAPI, post, \"/predict\"  \n",
                "**1.2**: mlflow, start_run, metric, model  \n",
                "**Q1**: Batch processes many at once (offline); real-time processes one at a time (online)  \n",
                "**Q2**: Reproducibility, portability, isolation  \n",
                "**Q3**: Model performance degrades over time; detect via monitoring accuracy on labeled samples\n",
                "\n",
                "</details>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}