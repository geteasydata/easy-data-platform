{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéÆ Reinforcement Learning\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê Advanced  \n",
                "**Time**: 90 minutes  \n",
                "**Prerequisites**: PyTorch, Probability\n",
                "\n",
                "## Learning Objectives\n",
                "- RL fundamentals (MDP, rewards)\n",
                "- Q-Learning and DQN\n",
                "- Policy Gradient methods\n",
                "- Stable-Baselines3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Q-Learning (Tabular)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple grid world\n",
                "n_states = 16  # 4x4 grid\n",
                "n_actions = 4  # up, down, left, right\n",
                "\n",
                "Q = np.zeros((n_states, n_actions))\n",
                "\n",
                "# Hyperparameters\n",
                "alpha = 0.1  # Learning rate\n",
                "gamma = 0.99  # Discount factor\n",
                "epsilon = 0.1  # Exploration rate\n",
                "\n",
                "def q_learning_update(state, action, reward, next_state):\n",
                "    \"\"\"Q-learning update rule.\"\"\"\n",
                "    best_next = np.max(Q[next_state])\n",
                "    Q[state, action] += alpha * (reward + gamma * best_next - Q[state, action])\n",
                "\n",
                "print(\"‚úÖ Q-Learning setup complete\")\n",
                "print(f\"Q-table shape: {Q.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Deep Q-Network (DQN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DQN(nn.Module):\n",
                "    def __init__(self, state_dim, action_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(state_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, action_dim)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "    \n",
                "    def select_action(self, state, epsilon):\n",
                "        if np.random.random() < epsilon:\n",
                "            return np.random.randint(self.net[-1].out_features)\n",
                "        with torch.no_grad():\n",
                "            q_values = self(torch.FloatTensor(state))\n",
                "            return q_values.argmax().item()\n",
                "\n",
                "dqn = DQN(4, 2)  # 4 state dims, 2 actions\n",
                "print(f\"DQN parameters: {sum(p.numel() for p in dqn.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Stable-Baselines3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sb3_example = '''\n",
                "from stable_baselines3 import PPO, DQN, A2C\n",
                "import gym\n",
                "\n",
                "# Create environment\n",
                "env = gym.make('CartPole-v1')\n",
                "\n",
                "# Train agent\n",
                "model = PPO('MlpPolicy', env, verbose=1)\n",
                "model.learn(total_timesteps=10000)\n",
                "\n",
                "# Evaluate\n",
                "obs = env.reset()\n",
                "for _ in range(1000):\n",
                "    action, _ = model.predict(obs)\n",
                "    obs, reward, done, info = env.step(action)\n",
                "    if done:\n",
                "        obs = env.reset()\n",
                "\n",
                "# Save\n",
                "model.save('ppo_cartpole')\n",
                "'''\n",
                "print(\"üìã Stable-Baselines3:\")\n",
                "print(sb3_example)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Algorithm Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "algorithms = pd.DataFrame({\n",
                "    'Algorithm': ['Q-Learning', 'DQN', 'A2C', 'PPO', 'SAC'],\n",
                "    'Type': ['Value', 'Value', 'Actor-Critic', 'Policy', 'Policy'],\n",
                "    'Actions': ['Discrete', 'Discrete', 'Both', 'Both', 'Continuous'],\n",
                "    'Best For': ['Small state', 'Atari', 'Simple tasks', 'General', 'Robotics']\n",
                "})\n",
                "\n",
                "display(algorithms)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "1. Start with PPO (robust, general)\n",
                "2. DQN for discrete, SAC for continuous\n",
                "3. Reward shaping is crucial\n",
                "4. Sim-to-real for robotics\n",
                "\n",
                "**Next**: 26_model_deployment_api.ipynb"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}