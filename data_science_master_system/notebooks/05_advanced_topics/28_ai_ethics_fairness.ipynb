{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# âš–ï¸ AI Ethics, Bias & Fairness\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: â­â­â­ Advanced  \n",
                "**Time**: 60 minutes  \n",
                "**Prerequisites**: ML Fundamentals\n",
                "\n",
                "## Learning Objectives\n",
                "- Understand bias in ML systems\n",
                "- Implement fairness metrics\n",
                "- Apply bias mitigation techniques\n",
                "- Build explainable AI systems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Types of Bias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bias_types = '''\n",
                "âš–ï¸ TYPES OF BIAS IN ML\n",
                "\n",
                "1. DATA BIAS\n",
                "   - Historical bias: Past discrimination in data\n",
                "   - Representation bias: Underrepresented groups\n",
                "   - Measurement bias: Different quality across groups\n",
                "\n",
                "2. ALGORITHMIC BIAS\n",
                "   - Optimization bias: Metric favors majority\n",
                "   - Aggregation bias: One model for all groups\n",
                "\n",
                "3. DEPLOYMENT BIAS\n",
                "   - Population shift: Different users than training\n",
                "   - Feedback loops: Predictions affect future data\n",
                "\n",
                "REAL-WORLD EXAMPLES:\n",
                "- Hiring algorithms favoring males\n",
                "- Facial recognition failing on darker skin\n",
                "- Loan approvals disadvantaging minorities\n",
                "- Recidivism prediction with racial disparities\n",
                "'''\n",
                "print(bias_types)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Fairness Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def demographic_parity(y_pred, protected_attr):\n",
                "    \"\"\"Selection rate should be equal across groups.\"\"\"\n",
                "    groups = np.unique(protected_attr)\n",
                "    rates = {}\n",
                "    for g in groups:\n",
                "        mask = protected_attr == g\n",
                "        rates[g] = y_pred[mask].mean()\n",
                "    return rates\n",
                "\n",
                "def equalized_odds(y_true, y_pred, protected_attr):\n",
                "    \"\"\"TPR and FPR should be equal across groups.\"\"\"\n",
                "    groups = np.unique(protected_attr)\n",
                "    metrics = {}\n",
                "    \n",
                "    for g in groups:\n",
                "        mask = protected_attr == g\n",
                "        cm = confusion_matrix(y_true[mask], y_pred[mask], labels=[0, 1])\n",
                "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
                "        \n",
                "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
                "        metrics[g] = {'TPR': tpr, 'FPR': fpr}\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "def disparate_impact(y_pred, protected_attr, privileged=1):\n",
                "    \"\"\"Ratio of selection rates (should be > 0.8).\"\"\"\n",
                "    rates = demographic_parity(y_pred, protected_attr)\n",
                "    groups = list(rates.keys())\n",
                "    \n",
                "    if len(groups) < 2:\n",
                "        return 1.0\n",
                "    \n",
                "    priv_rate = rates[privileged]\n",
                "    unpriv_rates = [rates[g] for g in groups if g != privileged]\n",
                "    \n",
                "    if priv_rate == 0:\n",
                "        return 0\n",
                "    \n",
                "    return min(unpriv_rates) / priv_rate\n",
                "\n",
                "print(\"ðŸ“Š Fairness Metrics:\")\n",
                "print(\"  - Demographic Parity: Equal selection rates\")\n",
                "print(\"  - Equalized Odds: Equal TPR and FPR\")\n",
                "print(\"  - Disparate Impact: Ratio > 0.8 (80% rule)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Bias Detection Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create biased dataset\n",
                "n = 1000\n",
                "protected = np.random.choice([0, 1], n, p=[0.3, 0.7])  # Group 0 is minority\n",
                "features = np.random.randn(n, 5)\n",
                "\n",
                "# Bias: Group 0 less likely to get positive outcome\n",
                "score = features[:, 0] + features[:, 1] + 0.5 * protected  # Protected group advantage\n",
                "labels = (score > np.median(score)).astype(int)\n",
                "\n",
                "# Train model\n",
                "X_train, X_test, y_train, y_test, p_train, p_test = train_test_split(\n",
                "    features, labels, protected, test_size=0.2\n",
                ")\n",
                "\n",
                "model = LogisticRegression()\n",
                "model.fit(X_train, y_train)\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Evaluate fairness\n",
                "print(\"ðŸ“Š Fairness Analysis:\")\n",
                "print(f\"\\nDemographic Parity: {demographic_parity(y_pred, p_test)}\")\n",
                "print(f\"\\nEqualized Odds: {equalized_odds(y_test, y_pred, p_test)}\")\n",
                "print(f\"\\nDisparate Impact: {disparate_impact(y_pred, p_test, privileged=1):.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Bias Mitigation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mitigation_strategies = pd.DataFrame({\n",
                "    'Stage': ['Pre-processing', 'Pre-processing', 'In-processing', 'Post-processing'],\n",
                "    'Technique': ['Reweighting', 'Resampling', 'Adversarial Debiasing', 'Threshold Adjustment'],\n",
                "    'Description': [\n",
                "        'Assign weights to balance groups',\n",
                "        'Over/undersample to balance',\n",
                "        'Train adversary to remove bias',\n",
                "        'Different thresholds per group'\n",
                "    ],\n",
                "    'Pros': [\n",
                "        'Simple',\n",
                "        'Easy to implement',\n",
                "        'Learned debiasing',\n",
                "        'No retraining'\n",
                "    ]\n",
                "})\n",
                "\n",
                "print(\"ðŸ”§ Bias Mitigation Strategies:\")\n",
                "display(mitigation_strategies)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Explainable AI (XAI)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xai_code = '''\n",
                "# SHAP for model explanations\n",
                "import shap\n",
                "\n",
                "explainer = shap.TreeExplainer(model)\n",
                "shap_values = explainer.shap_values(X_test)\n",
                "\n",
                "# Global feature importance\n",
                "shap.summary_plot(shap_values, X_test)\n",
                "\n",
                "# Local explanation for single prediction\n",
                "shap.force_plot(explainer.expected_value, shap_values[0], X_test[0])\n",
                "\n",
                "# LIME for any model\n",
                "from lime.lime_tabular import LimeTabularExplainer\n",
                "\n",
                "explainer = LimeTabularExplainer(X_train, feature_names=feature_names)\n",
                "exp = explainer.explain_instance(X_test[0], model.predict_proba)\n",
                "exp.show_in_notebook()\n",
                "'''\n",
                "print(\"ðŸ“‹ Explainable AI with SHAP & LIME:\")\n",
                "print(xai_code)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Ethical AI Framework"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "framework = '''\n",
                "ðŸ“‹ ETHICAL AI CHECKLIST\n",
                "\n",
                "â–¡ DATA STAGE\n",
                "  â–¡ Audit data for historical bias\n",
                "  â–¡ Check representation across groups\n",
                "  â–¡ Document data sources and limitations\n",
                "\n",
                "â–¡ DEVELOPMENT STAGE\n",
                "  â–¡ Define fairness metrics for the use case\n",
                "  â–¡ Test model on subgroups\n",
                "  â–¡ Document model behavior and limitations\n",
                "\n",
                "â–¡ DEPLOYMENT STAGE\n",
                "  â–¡ Monitor for drift in fairness metrics\n",
                "  â–¡ Provide explanations for decisions\n",
                "  â–¡ Enable human override for high-stakes decisions\n",
                "\n",
                "â–¡ GOVERNANCE\n",
                "  â–¡ Clear accountability structure\n",
                "  â–¡ Regular bias audits\n",
                "  â–¡ User feedback mechanisms\n",
                "'''\n",
                "print(framework)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Key Takeaways\n",
                "1. Bias can enter at any stage of ML pipeline\n",
                "2. Multiple fairness metrics exist - choose based on context\n",
                "3. Mitigation strategies: pre/in/post-processing\n",
                "4. XAI (SHAP, LIME) provides transparency\n",
                "5. Ethical AI requires ongoing monitoring\n",
                "\n",
                "**Libraries**: Fairlearn, AIF360, SHAP, LIME"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}