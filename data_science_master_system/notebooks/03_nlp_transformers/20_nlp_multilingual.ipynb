{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåç Multilingual NLP\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced  \n",
                "**Time**: 60 minutes  \n",
                "**Prerequisites**: 19_nlp_question_answering\n",
                "\n",
                "## Learning Objectives\n",
                "- Multilingual transformers (XLM-R, mBERT)\n",
                "- Cross-lingual transfer\n",
                "- Language detection\n",
                "- Translation with transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Multilingual Sentiment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from transformers import pipeline\n",
                "    \n",
                "    # Multilingual sentiment model\n",
                "    sentiment = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
                "    \n",
                "    texts = [\n",
                "        \"This is great!\",           # English\n",
                "        \"C'est magnifique!\",        # French\n",
                "        \"Das ist wunderbar!\",       # German\n",
                "        \"¬°Esto es incre√≠ble!\",      # Spanish\n",
                "        \"Ÿáÿ∞ÿß ÿ±ÿßÿ¶ÿπ!\"                  # Arabic\n",
                "    ]\n",
                "    \n",
                "    print(\"üåç Multilingual Sentiment:\")\n",
                "    for text in texts:\n",
                "        result = sentiment(text)[0]\n",
                "        print(f\"  '{text}' ‚Üí {result['label']}\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Demo: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Multilingual Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "models = pd.DataFrame({\n",
                "    'Model': ['mBERT', 'XLM-RoBERTa', 'mT5', 'NLLB'],\n",
                "    'Languages': ['104', '100+', '101', '200+'],\n",
                "    'Use Case': ['General NLU', 'Best cross-lingual', 'Generation', 'Translation'],\n",
                "    'Size': ['178M', '270M', '300M-13B', '600M-54B']\n",
                "})\n",
                "\n",
                "display(models)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Translation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    translator = pipeline('translation', model='Helsinki-NLP/opus-mt-en-fr')\n",
                "    \n",
                "    result = translator(\"Machine learning is transforming industries.\")\n",
                "    print(f\"Translation (EN‚ÜíFR): {result[0]['translation_text']}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Translation: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Cross-lingual Transfer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transfer_concept = '''\n",
                "Cross-lingual Transfer Learning:\n",
                "\n",
                "1. Train on English data only\n",
                "2. Zero-shot evaluation on other languages\n",
                "3. XLM-RoBERTa achieves 70-80% of supervised performance!\n",
                "\n",
                "Best practices:\n",
                "- Use XLM-RoBERTa for cross-lingual\n",
                "- Fine-tune on target language if data available\n",
                "- Combine with back-translation for data augmentation\n",
                "'''\n",
                "print(transfer_concept)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "1. XLM-RoBERTa: Best for cross-lingual\n",
                "2. Train on high-resource, transfer to low-resource\n",
                "3. Always test on target language\n",
                "4. Consider cultural context\n",
                "\n",
                "**Next**: 21_time_series_forecasting.ipynb"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}