{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚úçÔ∏è Text Generation: GPT & LLMs\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced  \n",
                "**Time**: 75 minutes  \n",
                "**Prerequisites**: 17_nlp_transformers_advanced\n",
                "\n",
                "## Learning Objectives\n",
                "- Understand autoregressive generation\n",
                "- Use GPT-2 and T5 models\n",
                "- Decoding strategies\n",
                "- Prompt engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. GPT-2 Text Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from transformers import pipeline\n",
                "    \n",
                "    generator = pipeline('text-generation', model='gpt2')\n",
                "    \n",
                "    result = generator(\n",
                "        \"The future of artificial intelligence is\",\n",
                "        max_length=50,\n",
                "        num_return_sequences=1,\n",
                "        temperature=0.7\n",
                "    )\n",
                "    \n",
                "    print(\"üìù Generated Text:\")\n",
                "    print(result[0]['generated_text'])\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"Install: pip install transformers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Decoding Strategies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "strategies = {\n",
                "    'Greedy': 'Always pick highest probability token',\n",
                "    'Beam Search': 'Keep top-k sequences, pick best',\n",
                "    'Sampling': 'Sample from probability distribution',\n",
                "    'Top-k': 'Sample from top k tokens only',\n",
                "    'Top-p (Nucleus)': 'Sample from smallest set with p probability mass',\n",
                "    'Temperature': 'Control randomness (lower = more focused)'\n",
                "}\n",
                "\n",
                "print(\"üéØ Decoding Strategies:\")\n",
                "for name, desc in strategies.items():\n",
                "    print(f\"  {name}: {desc}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare strategies\n",
                "try:\n",
                "    prompt = \"Machine learning is\"\n",
                "    \n",
                "    # Greedy (deterministic)\n",
                "    greedy = generator(prompt, max_length=30, do_sample=False)[0]['generated_text']\n",
                "    \n",
                "    # Top-p sampling (creative)\n",
                "    creative = generator(prompt, max_length=30, do_sample=True, top_p=0.9, temperature=0.8)[0]['generated_text']\n",
                "    \n",
                "    print(\"Greedy:\", greedy)\n",
                "    print(\"\\nCreative:\", creative)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Demo: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prompt Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompts = {\n",
                "    'Zero-shot': 'Classify: \"Great product!\" ‚Üí ',\n",
                "    'Few-shot': '''Classify sentiment:\n",
                "\"I love it\" ‚Üí Positive\n",
                "\"Terrible\" ‚Üí Negative\n",
                "\"Great product!\" ‚Üí ''',\n",
                "    'Chain-of-thought': '''Let's think step by step.\n",
                "Problem: Is \"Great product!\" positive or negative?\n",
                "1. The word \"Great\" is positive\n",
                "2. There's an exclamation mark showing enthusiasm\n",
                "3. Overall sentiment: '''\n",
                "}\n",
                "\n",
                "print(\"üìã Prompt Engineering Patterns:\")\n",
                "for name, prompt in prompts.items():\n",
                "    print(f\"\\n{name}:\")\n",
                "    print(f\"  {prompt[:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "1. Temperature controls creativity\n",
                "2. Top-p for diverse, coherent output\n",
                "3. Few-shot prompts improve accuracy\n",
                "4. Chain-of-thought for reasoning\n",
                "\n",
                "**Next**: 19_nlp_question_answering.ipynb"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}