{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü§ñ Advanced Transformers: BERT, RoBERTa & Beyond\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced  \n",
                "**Time**: 60 minutes  \n",
                "**Prerequisites**: 16_nlp_sentiment_analysis\n",
                "\n",
                "## Learning Objectives\n",
                "- Understand transformer architecture\n",
                "- Fine-tune BERT for classification\n",
                "- Compare BERT variants\n",
                "- Production deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Pretrained BERT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "    \n",
                "    model_name = 'bert-base-uncased'\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
                "    \n",
                "    print(f\"‚úÖ {model_name} loaded\")\n",
                "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"Install: pip install transformers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tokenization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"Transformers have revolutionized NLP!\"\n",
                "\n",
                "tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
                "\n",
                "print(\"üî§ Tokenization:\")\n",
                "print(f\"  Input: {text}\")\n",
                "print(f\"  Token IDs: {tokens['input_ids'][0][:10].tolist()}...\")\n",
                "print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][:10])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_code = '''\n",
                "from transformers import Trainer, TrainingArguments\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir='./results',\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=64,\n",
                "    learning_rate=2e-5,\n",
                "    warmup_steps=500,\n",
                "    weight_decay=0.01,\n",
                "    evaluation_strategy='epoch'\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=eval_dataset\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "'''\n",
                "print(\"üìã Fine-tuning Code:\")\n",
                "print(training_code)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['BERT-base', 'BERT-large', 'RoBERTa', 'DistilBERT', 'ALBERT', 'DeBERTa'],\n",
                "    'Params': ['110M', '340M', '125M', '66M', '12M', '134M'],\n",
                "    'Speed': ['1x', '0.3x', '1x', '2x', '1.5x', '0.8x'],\n",
                "    'GLUE': ['82.1', '85.2', '86.4', '79.0', '84.1', '88.8']\n",
                "})\n",
                "\n",
                "print(\"üìä Transformer Models:\")\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "1. Use DistilBERT for speed\n",
                "2. RoBERTa/DeBERTa for accuracy\n",
                "3. Learning rate ~2e-5 for fine-tuning\n",
                "4. 2-4 epochs usually sufficient\n",
                "\n",
                "**Next**: 18_nlp_text_generation.ipynb"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}