{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚ùì Question Answering Systems\n",
                "\n",
                "**Author**: Data Science Master System  \n",
                "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced  \n",
                "**Time**: 60 minutes  \n",
                "**Prerequisites**: 18_nlp_text_generation\n",
                "\n",
                "## Learning Objectives\n",
                "- Extractive vs Generative QA\n",
                "- Use BERT for extractive QA\n",
                "- Build RAG pipelines\n",
                "- Evaluate QA systems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Extractive QA with Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from transformers import pipeline\n",
                "    \n",
                "    qa = pipeline('question-answering')\n",
                "    \n",
                "    context = \"\"\"Machine learning is a subset of artificial intelligence that enables \n",
                "    systems to learn from data. It was pioneered by researchers like Arthur Samuel \n",
                "    in the 1950s. Today, ML powers recommendation systems, self-driving cars, and \n",
                "    natural language processing.\"\"\"\n",
                "    \n",
                "    questions = [\n",
                "        \"What is machine learning?\",\n",
                "        \"Who pioneered machine learning?\",\n",
                "        \"What does ML power today?\"\n",
                "    ]\n",
                "    \n",
                "    print(\"‚ùì Question Answering:\")\n",
                "    for q in questions:\n",
                "        result = qa(question=q, context=context)\n",
                "        print(f\"\\nQ: {q}\")\n",
                "        print(f\"A: {result['answer']} (confidence: {result['score']:.2%})\")\n",
                "        \n",
                "except ImportError:\n",
                "    print(\"Install: pip install transformers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. QA Types"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "qa_types = pd.DataFrame({\n",
                "    'Type': ['Extractive', 'Generative', 'Open-Domain', 'Multi-hop'],\n",
                "    'Method': ['Span extraction', 'Generate answer', 'Retrieve + Answer', 'Multiple docs'],\n",
                "    'Model': ['BERT', 'T5/GPT', 'RAG', 'HotpotQA'],\n",
                "    'Use Case': ['Document QA', 'Chatbots', 'Search', 'Complex reasoning']\n",
                "})\n",
                "\n",
                "display(qa_types)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. RAG (Retrieval-Augmented Generation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rag_pipeline = '''\n",
                "RAG Pipeline:\n",
                "\n",
                "1. User Question\n",
                "       ‚Üì\n",
                "2. Embed Question (sentence-transformers)\n",
                "       ‚Üì\n",
                "3. Search Vector DB (FAISS, Pinecone, Chroma)\n",
                "       ‚Üì\n",
                "4. Retrieve Top-K Documents\n",
                "       ‚Üì\n",
                "5. Combine: Question + Context\n",
                "       ‚Üì\n",
                "6. Generate Answer (LLM)\n",
                "       ‚Üì\n",
                "7. Return Answer + Sources\n",
                "'''\n",
                "print(rag_pipeline)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Key Takeaways\n",
                "1. Extractive: Fast, precise, limited\n",
                "2. Generative: Flexible, may hallucinate\n",
                "3. RAG: Best of both worlds\n",
                "4. Always cite sources in production\n",
                "\n",
                "**Next**: 20_nlp_multilingual.ipynb"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}