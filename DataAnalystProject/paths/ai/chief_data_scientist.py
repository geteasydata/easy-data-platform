"""
Chief Data Scientist Agent - The Expert Thinking Layer
========================================================
This agent does NOT clean data, write code, or build models.
It ONLY thinks, questions, rejects, and decides.

Rules:
- Prefer rejecting actions over doing them
- Simpler logic is better than complex models
- Reasoning quality is the main goal, not accuracy
- If data quality is weak, say so clearly
"""

from enum import Enum
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
import pandas as pd
import numpy as np


class ApprovalStatus(Enum):
    """Status of each thinking stage."""
    APPROVED = "approved"
    REJECTED = "rejected"
    NEEDS_REVIEW = "needs_review"


@dataclass
class ThinkingStageResult:
    """Result of a thinking stage."""
    status: ApprovalStatus
    reasoning: str
    concerns: List[str]
    recommendations: List[str]
    confidence: float  # 0.0 to 1.0


class ChiefDataScientist:
    """
    The Senior Expert Mind.
    
    Does NOT clean data.
    Does NOT write code.
    Does NOT build models.
    ONLY thinks, questions, rejects, and decides.
    
    CRITICAL: This agent has AUTHORITY to STOP execution.
    AutoML MUST NOT run unless this agent gives FULL APPROVAL.
    """
    
    # =========================================================================
    # HARD STOP CONDITIONS - NON-NEGOTIABLE
    # These are NOT warnings. These are STOP CONDITIONS.
    # =========================================================================
    HARD_STOP_CONDITIONS = {
        'min_rows': 100,                    # Dataset size < 100 rows
        'min_samples_per_feature': 5,       # Rows < 5 Ã— number of columns
        'max_missing_pct_per_col': 0.30,    # Any column has > 30% missing
        'max_leakage_correlation': 0.95,    # Target leakage risk
        'max_id_like_columns': 2,           # ID-like columns detected
    }
    
    def __init__(self, ai_ensemble=None):
        """Initialize with optional AI ensemble for LLM-powered thinking."""
        self.ai_ensemble = ai_ensemble
        self.thinking_log = []
        self.stage_results = {}
        self._fully_approved = False
        self._rejection_reasons = []
        
    def log(self, message: str):
        """Add to thinking log."""
        self.thinking_log.append(message)
        
    def get_log(self) -> List[str]:
        """Get all thinking log messages."""
        return self.thinking_log
    
    def is_fully_approved(self) -> bool:
        """
        EXPLICIT GATE: Check if ALL stages are APPROVED.
        
        AutoML MUST call this before running.
        If False, AutoML MUST NOT execute.
        """
        if not self.stage_results:
            return False
        
        for stage_name, result in self.stage_results.items():
            if result.status != ApprovalStatus.APPROVED:
                return False
        
        return self._fully_approved
    
    def get_rejection_summary(self, lang: str = 'ar') -> Dict[str, Any]:
        """
        Get summary of why analysis was rejected.
        This is shown INSTEAD of AutoML results.
        """
        summary = {
            'rejected': not self.is_fully_approved(),
            'reasons': self._rejection_reasons,
            'stage_details': {},
            'what_not_to_conclude': [],
            'what_senior_would_do': []
        }
        
        for stage_name, result in self.stage_results.items():
            summary['stage_details'][stage_name] = {
                'status': result.status.value,
                'reasoning': result.reasoning,
                'concerns': result.concerns
            }
            if result.status == ApprovalStatus.REJECTED:
                summary['reasons'].extend(result.concerns)
        
        # What NOT to conclude
        if lang == 'ar':
            summary['what_not_to_conclude'] = [
                "âŒ Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„",
                "âŒ Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ø£Ù† Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ø³ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
                "âŒ Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ù†ØªØ§Ø¦Ø¬ Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ«ÙˆÙ‚Ø©"
            ]
            summary['what_senior_would_do'] = [
                "1ï¸âƒ£ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¹Ù…Ù„ÙŠØ© Ø¬Ù…Ø¹Ù‡Ø§",
                "2ï¸âƒ£ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹",
                "3ï¸âƒ£ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±",
                "4ï¸âƒ£ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø¬Ø§Ù„ Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©"
            ]
        else:
            summary['what_not_to_conclude'] = [
                "âŒ Do NOT conclude that this data is suitable for analysis",
                "âŒ Do NOT conclude that any model will work on this data",
                "âŒ Do NOT draw statistical conclusions from unreliable data"
            ]
            summary['what_senior_would_do'] = [
                "1ï¸âƒ£ Review data source and collection process",
                "2ï¸âƒ£ Manually verify data quality",
                "3ï¸âƒ£ Collect additional data if necessary",
                "4ï¸âƒ£ Consult domain expert before proceeding"
            ]
        
        return summary
    
    # =========================================================================
    # EXPERT RECOVERY MODE
    # =========================================================================
    
    def generate_recovery_plan(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str = 'ar'
    ) -> Dict[str, Any]:
        """
        Generate a comprehensive recovery plan when analysis is rejected.
        
        A real senior data scientist NEVER just stops.
        They stop execution AND provide a recovery plan.
        """
        recovery = {
            'root_cause_diagnosis': [],
            'repair_plan': [],
            'auto_fix_candidates': {
                'safe': [],      # âœ” Can be automated
                'confirm': [],   # âš  Need human approval
                'never': []      # âŒ Must NEVER automate
            },
            're_entry_conditions': [],
            'domain_suggestions': []
        }
        
        n_samples = len(df)
        n_features = len(df.columns) - 1
        
        # =====================================================================
        # A. ROOT CAUSE DIAGNOSIS
        # =====================================================================
        for stage_name, result in self.stage_results.items():
            if result.status != ApprovalStatus.APPROVED:
                for concern in result.concerns:
                    diagnosis = self._diagnose_issue(concern, df, target_col, lang)
                    recovery['root_cause_diagnosis'].append(diagnosis)
        
        # =====================================================================
        # B. EXPERT REPAIR PLAN
        # =====================================================================
        for diagnosis in recovery['root_cause_diagnosis']:
            repair = self._generate_repair_options(diagnosis, df, target_col, lang)
            recovery['repair_plan'].append(repair)
        
        # =====================================================================
        # C. AUTO-FIX CANDIDATES CLASSIFICATION
        # =====================================================================
        recovery['auto_fix_candidates'] = self._classify_auto_fixes(df, target_col, lang)
        
        # =====================================================================
        # D. RE-ENTRY CONDITIONS
        # =====================================================================
        recovery['re_entry_conditions'] = self._define_re_entry_conditions(lang)
        
        # =====================================================================
        # E. DOMAIN-AWARE SUGGESTIONS
        # =====================================================================
        recovery['domain_suggestions'] = self._get_domain_suggestions(df, target_col, lang)
        
        return recovery
    
    def _diagnose_issue(
        self, 
        concern: str, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str
    ) -> Dict[str, Any]:
        """Diagnose a single issue with severity and statistical explanation."""
        
        # Determine severity
        if "HARD STOP" in concern or "LEAKAGE" in concern.upper():
            severity = "CRITICAL"
        elif "missing" in concern.lower() or "ID" in concern:
            severity = "MAJOR"
        else:
            severity = "MINOR"
        
        # Statistical explanation
        explanations = {
            'rows': "Small samples lead to high variance in model estimates, unreliable validation",
            'samples per feature': "Violates statistical rule of thumb, causes overfitting",
            'missing': "Missing data can bias model, imputation may introduce artifacts",
            'leakage': "Model learns target information, performance won't generalize",
            'ID': "ID columns have no predictive meaning, create spurious correlations",
            'imbalance': "Class imbalance biases accuracy metrics, may need resampling"
        }
        
        stat_reason = "General data quality concern"
        for key, explanation in explanations.items():
            if key.lower() in concern.lower():
                stat_reason = explanation
                break
        
        return {
            'concern': concern,
            'severity': severity,
            'statistical_reason': stat_reason,
            'severity_icon': 'ğŸ”´' if severity == 'CRITICAL' else ('ğŸŸ ' if severity == 'MAJOR' else 'ğŸŸ¡')
        }
    
    def _generate_repair_options(
        self, 
        diagnosis: Dict, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str
    ) -> Dict[str, Any]:
        """Generate repair options for a diagnosed issue."""
        
        concern = diagnosis['concern'].lower()
        repair = {
            'issue': diagnosis['concern'],
            'severity': diagnosis['severity'],
            'fix_conservative': '',
            'fix_aggressive': '',
            'risks_conservative': '',
            'risks_aggressive': '',
            'when_not_to_fix': ''
        }
        
        # Generate appropriate fixes based on issue type
        if 'rows' in concern or 'samples' in concern:
            repair['fix_conservative'] = "Collect more data (recommended: 10Ã— current size)"
            repair['fix_aggressive'] = "Use data augmentation or synthetic data generation"
            repair['risks_conservative'] = "Time and cost to collect data"
            repair['risks_aggressive'] = "Synthetic data may not represent real distribution"
            repair['when_not_to_fix'] = "When data collection is impossible or too expensive"
            
        elif 'missing' in concern:
            repair['fix_conservative'] = "Remove rows/columns with >30% missing"
            repair['fix_aggressive'] = "Impute using KNN or iterative imputation"
            repair['risks_conservative'] = "Loss of potentially useful data"
            repair['risks_aggressive'] = "Imputed values may introduce bias"
            repair['when_not_to_fix'] = "When missingness is informative (MNAR)"
            
        elif 'leakage' in concern:
            repair['fix_conservative'] = "Remove the leaking column entirely"
            repair['fix_aggressive'] = "Investigate if column is available at prediction time"
            repair['risks_conservative'] = "May lose genuinely useful feature"
            repair['risks_aggressive'] = "May still have subtle leakage"
            repair['when_not_to_fix'] = "When column IS legitimately available in production"
            
        elif 'id' in concern:
            repair['fix_conservative'] = "Remove all ID-like columns before modeling"
            repair['fix_aggressive'] = "Keep only if ID encodes meaningful information"
            repair['risks_conservative'] = "None - IDs should always be removed"
            repair['risks_aggressive'] = "High risk of spurious correlations"
            repair['when_not_to_fix'] = "Never - ID columns must always be removed"
            
        elif 'imbalance' in concern:
            repair['fix_conservative'] = "Use class weights in training"
            repair['fix_aggressive'] = "Apply SMOTE or undersampling"
            repair['risks_conservative'] = "May not fully address imbalance"
            repair['risks_aggressive'] = "SMOTE can create unrealistic samples"
            repair['when_not_to_fix'] = "When imbalance reflects real-world distribution"
            
        else:
            repair['fix_conservative'] = "Review data collection process"
            repair['fix_aggressive'] = "Consult domain expert for data cleaning"
            repair['risks_conservative'] = "May miss fixable issues"
            repair['risks_aggressive'] = "May over-engineer the data"
            repair['when_not_to_fix'] = "When issue is inherent to the problem"
        
        return repair
    
    def _classify_auto_fixes(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str
    ) -> Dict[str, List[Dict]]:
        """
        Classify fixes into safe/confirm/never categories.
        MUST match apply_safe_fixes logic!
        """
        
        classification = {
            'safe': [],      # âœ” Can be automated
            'confirm': [],   # âš  Need human approval
            'never': []      # âŒ Must NEVER automate
        }
        
        # =====================================================================
        # SAFE: ID columns (by name)
        # =====================================================================
        id_name_patterns = ['id', 'Id', 'ID', 'index', 'Index', 'INDEX', 'Unnamed: 0']
        for col in df.columns:
            if col in id_name_patterns and col != target_col:
                classification['safe'].append({
                    'action': f"Remove ID column '{col}'",
                    'reason': "ID provides no predictive value",
                    'code': f"df = df.drop(columns=['{col}'])"
                })
        
        # =====================================================================
        # SAFE: Columns with >30% missing (will be auto-removed - Hard Stop)
        # =====================================================================
        missing_pct = df.isnull().mean()
        for col in missing_pct[missing_pct > 0.30].index:
            if col != target_col:
                classification['safe'].append({
                    'action': f"Remove column '{col}' ({missing_pct[col]:.0%} missing)",
                    'reason': "Fails Hard Stop (>30%) - Must be removed",
                    'code': f"df = df.drop(columns=['{col}'])"
                })
        
        # =====================================================================
        # SAFE: Constant columns
        # =====================================================================
        for col in df.columns:
            if df[col].nunique() <= 1 and col != target_col:
                classification['safe'].append({
                    'action': f"Remove constant column '{col}'",
                    'reason': "Zero variance provides no information",
                    'code': f"df = df.drop(columns=['{col}'])"
                })
        
        # =====================================================================
        # SAFE: Remaining missing (<30%) will be filled
        # =====================================================================
        remaining_missing = missing_pct[(missing_pct > 0) & (missing_pct <= 0.30)]
        if len(remaining_missing) > 0:
            classification['safe'].append({
                'action': f"Fill missing in {len(remaining_missing)} columns",
                'reason': "Numeric â†’ median, Categorical â†’ mode",
                'code': "# Automatic imputation"
            })
        
        # =====================================================================
        # CONFIRM: Borderline cases (None in strict mode, but keeping structure)
        # =====================================================================
        # Currently empty as we handle >30% strictly
        
        # =====================================================================
        # NEVER: Target modifications
        # =====================================================================
        classification['never'].append({
            'action': "Modify target variable",
            'reason': "Target definition must be deliberate human decision"
        })
        classification['never'].append({
            'action': "Remove rows based on target value",
            'reason': "Would change problem definition and introduce bias"
        })
        classification['never'].append({
            'action': "Impute target variable",
            'reason': "Target must reflect ground truth, not estimates"
        })
        
        return classification
    
    def _define_re_entry_conditions(self, lang: str) -> List[Dict]:
        """Define conditions for re-running expert approval."""
        
        conditions = [
            {
                'condition': 'min_rows_met',
                'description': 'Dataset has â‰¥ 100 rows' if lang != 'ar' else 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 100 ØµÙ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„',
                'check': f"len(df) >= {self.HARD_STOP_CONDITIONS['min_rows']}"
            },
            {
                'condition': 'samples_per_feature_met',
                'description': 'At least 5 samples per feature' if lang != 'ar' else '5 Ø¹ÙŠÙ†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„ÙƒÙ„ Ù…ÙŠØ²Ø©',
                'check': f"len(df) / (len(df.columns)-1) >= {self.HARD_STOP_CONDITIONS['min_samples_per_feature']}"
            },
            {
                'condition': 'missing_rate_acceptable',
                'description': 'No column has > 30% missing' if lang != 'ar' else 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù…ÙˆØ¯ Ø¨Ù‡ Ø£ÙƒØ«Ø± Ù…Ù† 30% Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©',
                'check': "df.isnull().mean().max() <= 0.30"
            },
            {
                'condition': 'no_leakage',
                'description': 'No feature has > 0.95 correlation with target' if lang != 'ar' else 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙŠØ²Ø© Ø¨Ø§Ø±ØªØ¨Ø§Ø· > 0.95 Ù…Ø¹ Ø§Ù„Ù‡Ø¯Ù',
                'check': "max(correlations) < 0.95"
            },
            {
                'condition': 'id_columns_removed',
                'description': 'All ID-like columns removed' if lang != 'ar' else 'ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±Ù',
                'check': "no monotonic columns with unique values"
            }
        ]
        
        return conditions
    
    def _get_domain_suggestions(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str
    ) -> List[str]:
        """Get domain-aware suggestions based on problem type."""
        
        suggestions = []
        target = df[target_col] if target_col in df.columns else None
        n_samples = len(df)
        
        if target is None:
            return suggestions
        
        # Determine problem type
        if target.dtype in ['object', 'category'] or target.nunique() <= 20:
            problem_type = 'classification'
        else:
            problem_type = 'regression'
        
        # Classification-specific
        if problem_type == 'classification':
            class_counts = target.value_counts(normalize=True)
            if class_counts.iloc[0] > 0.8:
                suggestions.append("âš–ï¸ Class imbalance detected â†’ Consider SMOTE, class weights, or threshold tuning")
            if target.nunique() > 10:
                suggestions.append("ğŸ“Š Many classes â†’ Consider grouping rare classes or hierarchical classification")
        
        # Regression-specific
        if problem_type == 'regression':
            suggestions.append("ğŸ“ˆ Regression task â†’ Consider target transformations (log, Box-Cox) if skewed")
            suggestions.append("ğŸ¯ Use RMSE/MAE for evaluation, not just RÂ²")
        
        # Small data suggestions
        if n_samples < 500:
            suggestions.extend([
                "ğŸ“‰ Small dataset â†’ Use cross-validation instead of holdout",
                "ğŸŒ³ Prefer simple models: Logistic/Linear Regression, Decision Trees",
                "âŒ Avoid: Deep Learning, Large Ensembles, Neural Networks",
                "ğŸ“Š Consider: Regularization (L1/L2) to prevent overfitting"
            ])
        
        # High dimensional
        n_features = len(df.columns) - 1
        if n_features > n_samples / 5:
            suggestions.extend([
                "ğŸ“ High-dimensional â†’ Apply PCA or feature selection first",
                "ğŸ¯ Use Lasso (L1) regularization for automatic feature selection"
            ])
        
        return suggestions
    
    def apply_safe_fixes(
        self, 
        df: pd.DataFrame, 
        target_col: str
    ) -> Tuple[pd.DataFrame, List[str]]:
        """
        Apply safe fixes that a senior data scientist would approve automatically.
        
        SAFE to auto-fix:
        - ID columns (always remove)
        - Constant columns (zero information)
        - Columns with >80% missing (objectively useless)
        - Columns with >50% missing (likely not worth imputing)
        - Duplicate rows
        """
        changes = []
        df_fixed = df.copy()
        
        # =====================================================================
        # 1. REMOVE ID COLUMNS - Be conservative, only clear ID patterns
        # =====================================================================
        # Check for columns named like IDs
        id_name_patterns = ['id', 'Id', 'ID', 'index', 'Index', 'INDEX', 'Unnamed: 0']
        for col in list(df_fixed.columns):
            if col in id_name_patterns and col != target_col and col in df_fixed.columns:
                df_fixed = df_fixed.drop(columns=[col])
                changes.append(f"âœ”ï¸ Removed ID column: '{col}'")
        
        # Check for integer sequence columns (0,1,2,3... or 1,2,3,4...)
        for col in df_fixed.select_dtypes(include=[np.number]).columns[:10]:
            if col == target_col or col not in df_fixed.columns:
                continue
            col_data = df_fixed[col]
            # Must be all unique, monotonic, and be an exact sequence
            if col_data.nunique() == len(df_fixed):
                sorted_vals = sorted(col_data.dropna().values)
                is_sequence = all(sorted_vals[i] == sorted_vals[0] + i for i in range(len(sorted_vals)))
                if is_sequence and (sorted_vals[0] == 0 or sorted_vals[0] == 1):
                    df_fixed = df_fixed.drop(columns=[col])
                    changes.append(f"âœ”ï¸ Removed sequential ID column: '{col}'")
        
        # =====================================================================
        # 2. REMOVE COLUMNS WITH >30% MISSING (Strict Quality Control)
        # Why 30%? Because Chief Data Scientist sets a HARD STOP at 30%.
        # If we keep 30-50% missing, the analysis will just be rejected again.
        # =====================================================================
        missing_pct = df_fixed.isnull().mean()
        high_missing = missing_pct[missing_pct > 0.30].index.tolist()
        for col in high_missing:
            if col != target_col and col in df_fixed.columns:
                df_fixed = df_fixed.drop(columns=[col])
                changes.append(f"âœ”ï¸ Removed column '{col}' ({missing_pct[col]:.0%} missing - fails quality gate)")
        
        # =====================================================================
        # 3. FILL REMAINING MISSING VALUES (<30%)
        # =====================================================================
        # Recalculate only if needed, but we know all >30% are gone.
        remaining_missing_cols = df_fixed.columns[df_fixed.isnull().any()].tolist()
        
        # =====================================================================
        # 4. REMOVE CONSTANT COLUMNS
        # =====================================================================
        for col in list(df_fixed.columns):  # Use list() to avoid modification during iteration
            if col in df_fixed.columns and df_fixed[col].nunique() <= 1 and col != target_col:
                df_fixed = df_fixed.drop(columns=[col])
                changes.append(f"âœ”ï¸ Removed constant column: '{col}'")
        
        # =====================================================================
        # 5. REMOVE DUPLICATE ROWS
        # =====================================================================
        n_before = len(df_fixed)
        df_fixed = df_fixed.drop_duplicates()
        n_removed = n_before - len(df_fixed)
        if n_removed > 0:
            changes.append(f"âœ”ï¸ Removed {n_removed} duplicate rows")
        
        # =====================================================================
        # 6. FILL REMAINING MISSING VALUES (simple strategy)
        # =====================================================================
        for col in df_fixed.columns:
            if col == target_col:
                continue
            missing_count = df_fixed[col].isnull().sum()
            if missing_count > 0:
                if df_fixed[col].dtype in ['object', 'category']:
                    # Fill categorical with mode
                    mode_val = df_fixed[col].mode()
                    if len(mode_val) > 0:
                        df_fixed[col] = df_fixed[col].fillna(mode_val[0])
                        changes.append(f"âœ”ï¸ Filled {missing_count} missing in '{col}' with mode")
                else:
                    # Fill numeric with median
                    median_val = df_fixed[col].median()
                    df_fixed[col] = df_fixed[col].fillna(median_val)
                    changes.append(f"âœ”ï¸ Filled {missing_count} missing in '{col}' with median")
        
        return df_fixed, changes
    
    # =========================================================================
    # STAGE 1: PROBLEM REFRAMING
    # =========================================================================
    
    def stage1_problem_reframing(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str = 'ar'
    ) -> ThinkingStageResult:
        """
        Stage 1: Reframe the problem before any analysis.
        
        HARD STOP CONDITIONS CHECKED HERE:
        - Dataset size < 100 rows â†’ REJECT
        - Rows < 5 Ã— columns â†’ REJECT  
        - Target is ID-like â†’ REJECT
        """
        self.log("ğŸ§  Stage 1: Problem Reframing")
        
        concerns = []
        recommendations = []
        hard_stop = False
        
        n_samples = len(df)
        n_features = len(df.columns) - 1
        samples_per_feature = n_samples / max(n_features, 1)
        
        # Analyze target column
        target = df[target_col] if target_col in df.columns else None
        
        if target is None:
            self._rejection_reasons.append(f"HARD STOP: Target column '{target_col}' does not exist")
            return ThinkingStageResult(
                status=ApprovalStatus.REJECTED,
                reasoning=f"âŒ I REFUSE to analyze: Target column '{target_col}' not found.",
                concerns=["Target column does not exist"],
                recommendations=["Select a valid target column"],
                confidence=0.0
            )
        
        # =====================================================================
        # HARD STOP CONDITION 1: Dataset too small
        # =====================================================================
        if n_samples < self.HARD_STOP_CONDITIONS['min_rows']:
            hard_stop = True
            reason = f"HARD STOP: Dataset has only {n_samples} rows. Minimum required: {self.HARD_STOP_CONDITIONS['min_rows']}"
            concerns.append(reason)
            self._rejection_reasons.append(reason)
            recommendations.append("âŒ Using ML here would be STATISTICALLY IRRESPONSIBLE")
        
        # =====================================================================
        # HARD STOP CONDITION 2: Rows < 5 Ã— columns (severe underfitting risk)
        # =====================================================================
        if samples_per_feature < self.HARD_STOP_CONDITIONS['min_samples_per_feature']:
            hard_stop = True
            reason = f"HARD STOP: Only {samples_per_feature:.1f} samples per feature. Minimum required: {self.HARD_STOP_CONDITIONS['min_samples_per_feature']}"
            concerns.append(reason)
            self._rejection_reasons.append(reason)
            recommendations.append("âŒ Results would be MISLEADING due to overfitting")
        
        # =====================================================================
        # HARD STOP CONDITION 3: Target is ID-like (trivially predictable)
        # =====================================================================
        target_unique_ratio = target.nunique() / len(target)
        if target_unique_ratio > 0.95:
            hard_stop = True
            reason = "HARD STOP: Target appears to be an ID column (nearly unique values)"
            concerns.append(reason)
            self._rejection_reasons.append(reason)
            recommendations.append("âŒ This is NOT a valid prediction problem")
        
        # =====================================================================
        # WARNINGS (not hard stops, but concerning)
        # =====================================================================
        if target.dtype in ['object', 'category'] or target.nunique() <= 10:
            value_counts = target.value_counts(normalize=True)
            if value_counts.iloc[0] > 0.9:
                concerns.append(f"âš ï¸ Target is heavily imbalanced: {value_counts.iloc[0]:.1%} in majority class")
        
        if n_features <= 3 and target.nunique() <= 5:
            recommendations.append("ğŸ’¡ Simple rules might work better than ML")
        
        # Generate reasoning with AI if available
        if hard_stop:
            if lang == 'ar':
                reasoning = f"âŒ **Ø£Ø±ÙØ¶ ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.**\n\nØ§Ù„Ø£Ø³Ø¨Ø§Ø¨:\n" + "\n".join([f"â€¢ {c}" for c in concerns])
            else:
                reasoning = f"âŒ **I REFUSE to analyze this data.**\n\nReasons:\n" + "\n".join([f"â€¢ {c}" for c in concerns])
        else:
            reasoning = self._generate_problem_reframing_reasoning(df, target_col, concerns, lang)
        
        # =====================================================================
        # DETERMINE STATUS
        # =====================================================================
        if hard_stop:
            status = ApprovalStatus.REJECTED
            confidence = 0.0
        elif len(concerns) >= 2:
            status = ApprovalStatus.REJECTED  # Changed: 2+ concerns = REJECT, not NEEDS_REVIEW
            confidence = 0.3
        else:
            status = ApprovalStatus.APPROVED
            confidence = 0.8 - (len(concerns) * 0.15)
        
        result = ThinkingStageResult(
            status=status,
            reasoning=reasoning,
            concerns=concerns,
            recommendations=recommendations,
            confidence=max(0.0, confidence)
        )
        
        self.stage_results['problem_reframing'] = result
        return result
    
    # =========================================================================
    # STAGE 2: DATA SKEPTICISM
    # =========================================================================
    
    def stage2_data_skepticism(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str = 'ar'
    ) -> ThinkingStageResult:
        """
        Stage 2: Question the dataset validity.
        
        HARD STOP CONDITIONS CHECKED HERE:
        - Any column has > 30% missing â†’ REJECT
        - Data leakage detected (correlation > 0.95) â†’ REJECT
        - ID-like columns dominate variance â†’ REJECT
        """
        self.log("ğŸ” Stage 2: Data Skepticism & Quality Check")
        
        concerns = []
        recommendations = []
        hard_stop = False
        id_like_count = 0
        
        # =====================================================================
        # HARD STOP CONDITION 1: Any column has > 30% missing
        # =====================================================================
        missing_pct = df.isnull().mean()
        high_missing_cols = missing_pct[missing_pct > self.HARD_STOP_CONDITIONS['max_missing_pct_per_col']].index.tolist()
        if high_missing_cols:
            hard_stop = True
            reason = f"HARD STOP: {len(high_missing_cols)} columns have >30% missing data: {high_missing_cols[:3]}"
            concerns.append(reason)
            self._rejection_reasons.append(reason)
            recommendations.append("âŒ Data quality is TOO POOR for reliable analysis")
        
        # =====================================================================
        # HARD STOP CONDITION 2: Data leakage detected
        # =====================================================================
        if target_col in df.columns:
            target = df[target_col]
            for col in df.columns:
                if col == target_col:
                    continue
                if df[col].dtype in ['object', 'category']:
                    continue
                try:
                    corr = df[col].corr(target.astype(float))
                    if abs(corr) > self.HARD_STOP_CONDITIONS['max_leakage_correlation']:
                        hard_stop = True
                        reason = f"HARD STOP: DATA LEAKAGE - '{col}' has {corr:.2f} correlation with target"
                        concerns.append(reason)
                        self._rejection_reasons.append(reason)
                        recommendations.append(f"âŒ '{col}' is likely DERIVED from the target - analysis would be FRAUDULENT")
                except:
                    pass
        
        # =====================================================================
        # HARD STOP CONDITION 3: ID-like columns dominate
        # =====================================================================
        for col in df.select_dtypes(include=[np.number]).columns[:15]:
            if df[col].is_monotonic_increasing or df[col].is_monotonic_decreasing:
                if df[col].nunique() == len(df):
                    id_like_count += 1
                    concerns.append(f"âš ï¸ '{col}' is a sequential ID - MUST be excluded")
        
        if id_like_count > self.HARD_STOP_CONDITIONS['max_id_like_columns']:
            hard_stop = True
            reason = f"HARD STOP: {id_like_count} ID-like columns detected dominating the data"
            concerns.append(reason)
            self._rejection_reasons.append(reason)
            recommendations.append("âŒ Remove all ID columns before analysis")
        
        # =====================================================================
        # WARNINGS (not hard stops)
        # =====================================================================
        dup_pct = df.duplicated().mean()
        if dup_pct > 0.05:
            concerns.append(f"âš ï¸ {dup_pct:.1%} duplicate rows detected")
        
        constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
        if constant_cols:
            concerns.append(f"âš ï¸ {len(constant_cols)} columns have no variance (constant)")
        
        # Generate reasoning
        if hard_stop:
            if lang == 'ar':
                reasoning = f"âŒ **Ø£Ø±ÙØ¶ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ«ÙˆÙ‚Ø©.**\n\n" + "\n".join([f"â€¢ {c}" for c in concerns])
            else:
                reasoning = f"âŒ **I REFUSE to proceed - data is UNRELIABLE.**\n\n" + "\n".join([f"â€¢ {c}" for c in concerns])
        else:
            reasoning = self._generate_data_skepticism_reasoning(df, target_col, concerns, lang)
        
        # =====================================================================
        # DETERMINE STATUS - STRICT
        # =====================================================================
        if hard_stop:
            status = ApprovalStatus.REJECTED
            confidence = 0.0
        elif len(concerns) >= 3:
            status = ApprovalStatus.REJECTED  # 3+ concerns = REJECT
            confidence = 0.2
        elif len(concerns) >= 1:
            status = ApprovalStatus.APPROVED  # 1-2 concerns = APPROVED with lower confidence
            confidence = 0.6 - (len(concerns) * 0.15)
        else:
            status = ApprovalStatus.APPROVED
            confidence = 0.85
        
        result = ThinkingStageResult(
            status=status,
            reasoning=reasoning,
            concerns=concerns,
            recommendations=recommendations,
            confidence=max(0.0, confidence)
        )
        
        self.stage_results['data_skepticism'] = result
        return result
    
    # =========================================================================
    # STAGE 3: ANALYSIS STRATEGY
    # =========================================================================
    
    def stage3_analysis_strategy(
        self, 
        df: pd.DataFrame, 
        target_col: str,
        lang: str = 'ar'
    ) -> ThinkingStageResult:
        """
        Stage 3: Decide the analysis approach BEFORE doing it.
        
        Questions to answer:
        - What modeling approach is appropriate?
        - What should we NOT do?
        - What validation strategy is suitable?
        - What are the success criteria?
        """
        self.log("ğŸ“‹ Stage 3: Analysis Strategy Decision")
        
        concerns = []
        recommendations = []
        
        n_samples = len(df)
        n_features = len(df.columns) - 1
        target = df[target_col] if target_col in df.columns else None
        
        # Determine problem type
        if target is not None:
            if target.dtype in ['object', 'category'] or target.nunique() <= 20:
                problem_type = 'classification'
                n_classes = target.nunique()
            else:
                problem_type = 'regression'
                n_classes = None
        else:
            problem_type = 'unknown'
            n_classes = None
        
        # Strategy decisions
        strategy = {
            'problem_type': problem_type,
            'recommended_models': [],
            'avoid_models': [],
            'validation_strategy': '',
            'success_criteria': []
        }
        
        # Model recommendations based on data characteristics
        if n_samples < 1000:
            recommendations.append("Small dataset: prefer simpler models (Logistic Regression, Decision Trees)")
            strategy['recommended_models'] = ['Logistic Regression', 'Decision Tree', 'Random Forest']
            strategy['avoid_models'] = ['Deep Learning', 'Large Ensembles']
            concerns.append("Dataset may be too small for complex models to generalize")
        elif n_samples < 10000:
            recommendations.append("Medium dataset: tree-based models should work well")
            strategy['recommended_models'] = ['Random Forest', 'Gradient Boosting', 'XGBoost']
        else:
            recommendations.append("Large dataset: can consider more complex approaches")
            strategy['recommended_models'] = ['XGBoost', 'LightGBM', 'Neural Networks']
        
        # Validation strategy
        if n_samples < 500:
            strategy['validation_strategy'] = 'Leave-One-Out or 10-Fold CV (small sample)'
            concerns.append("Small sample size limits validation reliability")
        elif n_samples < 5000:
            strategy['validation_strategy'] = '5-Fold Cross-Validation'
        else:
            strategy['validation_strategy'] = 'Train/Validation/Test Split (70/15/15)'
        
        # Success criteria
        if problem_type == 'classification':
            if n_classes and n_classes == 2:
                strategy['success_criteria'] = [
                    'AUC-ROC as primary metric (handles imbalance better)',
                    'Precision/Recall based on business cost of errors',
                    'Confusion matrix analysis for error patterns'
                ]
            else:
                strategy['success_criteria'] = [
                    'Macro F1-Score for balanced class importance',
                    'Per-class metrics to identify weak spots'
                ]
        else:
            strategy['success_criteria'] = [
                'RMSE for absolute error magnitude',
                'RÂ² for explained variance',
                'Residual analysis for model assumptions'
            ]
        
        # What NOT to do
        if n_features > n_samples / 10:
            concerns.append("âš ï¸ High-dimensional data: feature selection is CRITICAL")
            recommendations.append("Apply aggressive feature selection before modeling")
        
        # Generate reasoning with AI if available
        reasoning = self._generate_strategy_reasoning(df, target_col, strategy, concerns, lang)
        
        # Store strategy for later use
        self.analysis_strategy = strategy
        
        # =====================================================================
        # DETERMINE STATUS - STRICT (no NEEDS_REVIEW - either APPROVED or REJECTED)
        # =====================================================================
        if problem_type == 'unknown':
            status = ApprovalStatus.REJECTED
            confidence = 0.0
            self._rejection_reasons.append("REJECTED: Cannot determine problem type")
        elif len(concerns) >= 3:
            status = ApprovalStatus.REJECTED  # Changed: 3+ concerns = REJECT
            confidence = 0.3
            self._rejection_reasons.append(f"REJECTED: Too many concerns ({len(concerns)}) for reliable analysis")
        else:
            status = ApprovalStatus.APPROVED
            confidence = 0.75 - (len(concerns) * 0.1)
        
        result = ThinkingStageResult(
            status=status,
            reasoning=reasoning,
            concerns=concerns,
            recommendations=recommendations,
            confidence=max(0.0, confidence)
        )
        
        self.stage_results['analysis_strategy'] = result
        
        # =====================================================================
        # SET FULLY APPROVED FLAG
        # Only True if ALL THREE stages are APPROVED
        # =====================================================================
        all_approved = all(
            r.status == ApprovalStatus.APPROVED 
            for r in self.stage_results.values()
        )
        self._fully_approved = all_approved
        
        if not all_approved:
            self.log("âŒ EXECUTION BLOCKED: Not all stages approved")
        else:
            self.log("âœ… ALL STAGES APPROVED: AutoML may proceed")
        
        return result
    
    # =========================================================================
    # SELF-CRITIQUE STAGE (POST-ANALYSIS)
    # =========================================================================
    
    def generate_self_critique(
        self,
        results: Dict[str, Any],
        lang: str = 'ar'
    ) -> Dict[str, Any]:
        """
        Generate self-critique of the analysis results.
        
        Challenges:
        - Weak assumptions made
        - How the analysis could be wrong
        - Overconfidence warnings
        """
        self.log("âš–ï¸ Self-Critique Stage")
        
        critique = {
            'weak_assumptions': [],
            'potential_errors': [],
            'overconfidence_warnings': [],
            'expert_warnings': [],
            'confidence_level': 'medium'
        }
        
        metrics = results.get('metrics', {})
        problem_type = results.get('problem_type', '')
        best_model = results.get('best_model', '')
        
        # Check 1: Accuracy illusion
        if problem_type == 'classification':
            acc = metrics.get('accuracy', 0)
            if acc > 0.95:
                critique['overconfidence_warnings'].append(
                    f"Accuracy of {acc:.1%} is suspiciously high - possible data leakage or overfitting"
                )
                critique['confidence_level'] = 'low'
            elif acc > 0.85:
                critique['weak_assumptions'].append(
                    "High accuracy may not transfer to production data"
                )
        
        # Check 2: Model complexity concerns
        if 'Ensemble' in best_model or 'XGBoost' in best_model or 'LightGBM' in best_model:
            critique['potential_errors'].append(
                "Complex model selected - may overfit to training patterns"
            )
            critique['expert_warnings'].append(
                "A senior data scientist would recommend testing with simpler baselines first"
            )
        
        # Check 3: Validation concerns
        critique['weak_assumptions'].append(
            "We assume the test/train split is representative of future data"
        )
        critique['weak_assumptions'].append(
            "We assume no temporal drift or distribution shift in production"
        )
        
        # Check 4: Feature importance concerns
        feature_importance = results.get('feature_importance')
        if feature_importance is not None and len(feature_importance) > 0:
            top_feature = feature_importance.iloc[0]['Feature'] if isinstance(feature_importance, pd.DataFrame) else 'Unknown'
            top_importance = feature_importance.iloc[0]['Importance'] if isinstance(feature_importance, pd.DataFrame) else 0
            if top_importance > 0.5:
                critique['potential_errors'].append(
                    f"Model heavily relies on '{top_feature}' ({top_importance:.1%}) - is this feature always available?"
                )
        
        # Generate expert warnings with AI if available
        if self.ai_ensemble:
            try:
                ai_warnings = self._generate_ai_critique(results, lang)
                if ai_warnings:
                    critique['expert_warnings'].extend(ai_warnings)
            except:
                pass
        
        # Add standard expert warnings
        critique['expert_warnings'].extend([
            "Always validate on truly held-out data before deployment",
            "Monitor model performance continuously in production",
            "This analysis is a starting point, not a final answer"
        ])
        
        return critique
    
    # =========================================================================
    # EXPERT OUTPUT FORMATTING
    # =========================================================================
    
    def format_expert_output(
        self,
        results: Dict[str, Any],
        critique: Dict[str, Any],
        lang: str = 'ar'
    ) -> Dict[str, str]:
        """
        Format the output like a senior data scientist would present it.
        """
        output = {}
        
        # Expert interpretation
        interpretation = self._generate_expert_interpretation(results, lang)
        output['expert_interpretation'] = interpretation
        
        # Practical recommendations
        recommendations = self._generate_practical_recommendations(results, lang)
        output['practical_recommendations'] = recommendations
        
        # Uncertainty and risk
        output['uncertainty_statement'] = self._generate_uncertainty_statement(results, critique, lang)
        
        # Senior warnings
        output['senior_warnings'] = "\n".join([f"âš ï¸ {w}" for w in critique['expert_warnings']])
        
        return output
    
    # =========================================================================
    # HELPER METHODS FOR AI-POWERED REASONING
    # =========================================================================
    
    def _generate_problem_reframing_reasoning(
        self, df: pd.DataFrame, target_col: str, concerns: List[str], lang: str
    ) -> str:
        """Generate reasoning for problem reframing stage."""
        if self.ai_ensemble and hasattr(self.ai_ensemble, '_call_groq'):
            try:
                prompt = f"""Ø£Ù†Øª ÙƒØ¨ÙŠØ± Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø¥ÙŠØ¬Ø§Ø²:
                
Ø§Ù„Ù‡Ø¯Ù: {target_col}
Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {len(df.columns)}
Ø§Ù„ØµÙÙˆÙ: {len(df)}
Ø§Ù„Ù…Ø®Ø§ÙˆÙ: {concerns}

Ø£Ø¬Ø¨ ÙÙŠ 2-3 Ø¬Ù…Ù„ ÙÙ‚Ø·. Ø±ÙƒØ² Ø¹Ù„Ù‰: Ù‡Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„Ø­Ù„ØŸ
Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}"""
                return self.ai_ensemble._call_groq(prompt)
            except:
                pass
        
        # Fallback reasoning
        if lang == 'ar':
            return f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¯Ù '{target_col}' Ø¹Ù„Ù‰ {len(df)} ØµÙ Ùˆ {len(df.columns)} Ø¹Ù…ÙˆØ¯. " + \
                   ("ØªÙˆØ¬Ø¯ Ù…Ø®Ø§ÙˆÙ ÙŠØ¬Ø¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§." if concerns else "Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„.")
        else:
            return f"Analyzing target '{target_col}' on {len(df)} rows and {len(df.columns)} columns. " + \
                   ("There are concerns that need attention." if concerns else "Problem is clear and suitable for analysis.")
    
    def _generate_data_skepticism_reasoning(
        self, df: pd.DataFrame, target_col: str, concerns: List[str], lang: str
    ) -> str:
        """Generate reasoning for data skepticism stage."""
        if self.ai_ensemble and hasattr(self.ai_ensemble, '_call_groq'):
            try:
                prompt = f"""Ø£Ù†Øª ÙƒØ¨ÙŠØ± Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù‚ÙŠÙ‘Ù… Ø¬ÙˆØ¯Ø© Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¥ÙŠØ¬Ø§Ø²:

Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: {df.isnull().sum().sum()}
Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª: {df.duplicated().sum()}
Ø§Ù„Ù…Ø®Ø§ÙˆÙ: {concerns}

Ø£Ø¬Ø¨ ÙÙŠ 2-3 Ø¬Ù…Ù„ ÙÙ‚Ø·. Ø±ÙƒØ² Ø¹Ù„Ù‰: Ù‡Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø©ØŸ
Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}"""
                return self.ai_ensemble._call_groq(prompt)
            except:
                pass
        
        # Fallback reasoning
        missing_pct = df.isnull().mean().mean() * 100
        if lang == 'ar':
            quality = "Ø¬ÙŠØ¯Ø©" if missing_pct < 5 else ("Ù…ØªÙˆØ³Ø·Ø©" if missing_pct < 20 else "Ø¶Ø¹ÙŠÙØ©")
            return f"Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª {quality}. Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_pct:.1f}%. " + \
                   (f"ØªÙ… Ø±ØµØ¯ {len(concerns)} Ù…Ø®Ø§ÙˆÙ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©." if concerns else "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¨Ø¯Ùˆ Ø³Ù„ÙŠÙ…Ø©.")
        else:
            quality = "good" if missing_pct < 5 else ("moderate" if missing_pct < 20 else "poor")
            return f"Data quality is {quality}. Missing values: {missing_pct:.1f}%. " + \
                   (f"Found {len(concerns)} concerns requiring review." if concerns else "Data appears clean.")
    
    def _generate_strategy_reasoning(
        self, df: pd.DataFrame, target_col: str, strategy: Dict, concerns: List[str], lang: str
    ) -> str:
        """Generate reasoning for analysis strategy stage."""
        if self.ai_ensemble and hasattr(self.ai_ensemble, '_call_groq'):
            try:
                prompt = f"""Ø£Ù†Øª ÙƒØ¨ÙŠØ± Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø§Ù‚ØªØ±Ø­ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø¥ÙŠØ¬Ø§Ø²:

Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: {strategy['problem_type']}
Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©: {strategy['recommended_models']}
Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¬Ù†Ø¨Ø©: {strategy['avoid_models']}

Ø£Ø¬Ø¨ ÙÙŠ 2-3 Ø¬Ù…Ù„ ÙÙ‚Ø·. Ø±ÙƒØ² Ø¹Ù„Ù‰: Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©ØŸ
Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}"""
                return self.ai_ensemble._call_groq(prompt)
            except:
                pass
        
        # Fallback reasoning
        if lang == 'ar':
            return f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: {strategy['problem_type']}. " + \
                   f"Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©: {', '.join(strategy['recommended_models'][:3])}. " + \
                   f"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ­Ù‚Ù‚: {strategy['validation_strategy']}."
        else:
            return f"Problem type: {strategy['problem_type']}. " + \
                   f"Recommended models: {', '.join(strategy['recommended_models'][:3])}. " + \
                   f"Validation: {strategy['validation_strategy']}."
    
    def _generate_ai_critique(self, results: Dict, lang: str) -> List[str]:
        """Generate AI-powered critique."""
        if not self.ai_ensemble:
            return []
        
        try:
            prompt = f"""Ø£Ù†Øª ÙƒØ¨ÙŠØ± Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø§Ù†ØªÙ‚Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:

Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {results.get('best_model', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
Ø§Ù„Ø¯Ù‚Ø©: {results.get('metrics', {}).get('accuracy', results.get('metrics', {}).get('r2', 'N/A'))}

Ø£Ø¹Ø·Ù†ÙŠ 2 ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø© ÙÙ‚Ø·. ÙƒÙ„ ØªØ­Ø°ÙŠØ± ÙÙŠ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯.
Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}"""
            
            response = self.ai_ensemble._call_groq(prompt)
            return [line.strip() for line in response.split('\n') if line.strip()][:2]
        except:
            return []
    
    def _generate_expert_interpretation(self, results: Dict, lang: str) -> str:
        """Generate expert interpretation of results."""
        metrics = results.get('metrics', {})
        problem_type = results.get('problem_type', '')
        best_model = results.get('best_model', '')
        
        if problem_type == 'classification':
            acc = metrics.get('accuracy', 0)
            if lang == 'ar':
                if acc > 0.85:
                    return f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­Ù‚Ù‚ Ø¯Ù‚Ø© {acc:.1%} ÙˆÙ‡ÙŠ Ù†ØªÙŠØ¬Ø© Ø¬ÙŠØ¯Ø©ØŒ Ù„ÙƒÙ† ÙŠØ¬Ø¨ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ØªØ³Ø±Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª."
                elif acc > 0.7:
                    return f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­Ù‚Ù‚ Ø¯Ù‚Ø© {acc:.1%} ÙˆÙ‡ÙŠ Ù†ØªÙŠØ¬Ø© Ù…Ù‚Ø¨ÙˆÙ„Ø© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©."
                else:
                    return f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­Ù‚Ù‚ Ø¯Ù‚Ø© {acc:.1%} ÙÙ‚Ø· - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† ÙƒØ¨ÙŠØ± Ø£Ùˆ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."
            else:
                if acc > 0.85:
                    return f"Model achieves {acc:.1%} accuracy - good result, but verify no data leakage."
                elif acc > 0.7:
                    return f"Model achieves {acc:.1%} accuracy - acceptable for initial model."
                else:
                    return f"Model achieves only {acc:.1%} accuracy - needs significant improvement or data quality review."
        else:
            r2 = metrics.get('r2', 0)
            if lang == 'ar':
                return f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙØ³Ø± {r2:.1%} Ù…Ù† Ø§Ù„ØªØ¨Ø§ÙŠÙ† ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."
            else:
                return f"Model explains {r2:.1%} of variance in the data."
    
    def _generate_practical_recommendations(self, results: Dict, lang: str) -> str:
        """Generate practical recommendations."""
        if lang == 'ar':
            return """
1. ğŸ“Š **Ø§Ø®ØªØ¨Ø± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©** Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
2. ğŸ”„ **Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡** Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
3. ğŸ“‰ **Ø§Ø¨Ø¯Ø£ Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ·** ÙƒØ®Ø· Ø£Ø³Ø§Ø³ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
4. ğŸ¯ **Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¹Ù…Ù„** ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ©
"""
        else:
            return """
1. ğŸ“Š **Test on new data** before relying on results
2. ğŸ”„ **Monitor performance** regularly in production
3. ğŸ“‰ **Start with simple model** as baseline for comparison
4. ğŸ¯ **Focus on business metrics**, not just technical accuracy
"""
    
    def _generate_uncertainty_statement(
        self, results: Dict, critique: Dict, lang: str
    ) -> str:
        """Generate statement about uncertainty and risk."""
        confidence = critique.get('confidence_level', 'medium')
        n_warnings = len(critique.get('overconfidence_warnings', []))
        
        if lang == 'ar':
            if confidence == 'low' or n_warnings > 0:
                return "âš ï¸ **Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: Ù…Ù†Ø®ÙØ¶** - ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª ØªØ³ØªØ¯Ø¹ÙŠ Ø§Ù„Ø­Ø°Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬."
            elif confidence == 'medium':
                return "ğŸ“Š **Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: Ù…ØªÙˆØ³Ø·** - Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹Ù‚ÙˆÙ„Ø© Ù„ÙƒÙ† ØªØ­ØªØ§Ø¬ ØªØ­Ù‚Ù‚Ø§Ù‹ Ø¥Ø¶Ø§ÙÙŠØ§Ù‹."
            else:
                return "âœ… **Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: Ù…Ù‚Ø¨ÙˆÙ„** - Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ¨Ø¯Ùˆ Ø³Ù„ÙŠÙ…Ø© Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§Ø°ÙŠØ± Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©."
        else:
            if confidence == 'low' or n_warnings > 0:
                return "âš ï¸ **Confidence: LOW** - There are indicators that warrant caution before relying on these results."
            elif confidence == 'medium':
                return "ğŸ“Š **Confidence: MEDIUM** - Results are reasonable but need additional validation."
            else:
                return "âœ… **Confidence: ACCEPTABLE** - Results appear sound with noted caveats."


# =========================================================================
# FACTORY FUNCTION
# =========================================================================

def get_chief_data_scientist(ai_ensemble=None) -> ChiefDataScientist:
    """Get a ChiefDataScientist instance."""
    return ChiefDataScientist(ai_ensemble=ai_ensemble)
