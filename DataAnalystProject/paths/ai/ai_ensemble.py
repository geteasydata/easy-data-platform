"""
AI Ensemble - Combines Multiple AI Providers for Superior Analysis
Automatically uses Groq, DeepSeek, and Gemini together
"""

import os
from typing import Dict, Any, Optional, List
import pandas as pd
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# Try to import Groq
try:
    from groq import Groq
    HAS_GROQ = True
except ImportError:
    HAS_GROQ = False

# Try to import Gemini
try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False

# Try to import requests for DeepSeek
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False


class AIEnsemble:
    """
    AI Ensemble Engine - Uses ALL available AI providers together.
    Automatically combines insights from Groq, DeepSeek, and Gemini.
    """
    
    # Default API keys (user provided)
    DEFAULT_GROQ_KEY = os.environ.get("GROQ_API_KEY") or st.secrets.get("GROQ_API_KEY")
    DEFAULT_DEEPSEEK_KEY = os.environ.get("DEEPSEEK_API_KEY") or st.secrets.get("DEEPSEEK_API_KEY")
    DEFAULT_GEMINI_KEY = os.environ.get("GEMINI_API_KEY") or st.secrets.get("GEMINI_API_KEY")
    
    def __init__(self):
        """Initialize all AI providers automatically."""
        self.providers = {}
        self.log_messages = []
        
        # Setup Groq
        if HAS_GROQ:
            try:
                self.providers['groq'] = {
                    'client': Groq(api_key=self.DEFAULT_GROQ_KEY),
                    'name': 'Groq (Llama 3.3)',
                    'emoji': 'ðŸš€'
                }
                self.log("âœ… Groq Ù…ØªØµÙ„")
            except Exception as e:
                self.log(f"âš ï¸ Groq: {e}")
        
        # Setup DeepSeek
        if HAS_REQUESTS:
            self.providers['deepseek'] = {
                'key': self.DEFAULT_DEEPSEEK_KEY,
                'name': 'DeepSeek',
                'emoji': 'ðŸ”®'
            }
            self.log("âœ… DeepSeek Ù…ØªØµÙ„")
        
        # Setup Gemini
        if HAS_GEMINI:
            try:
                genai.configure(api_key=self.DEFAULT_GEMINI_KEY)
                self.providers['gemini'] = {
                    'model': genai.GenerativeModel('gemini-pro'),
                    'name': 'Gemini',
                    'emoji': 'âœ¨'
                }
                self.log("âœ… Gemini Ù…ØªØµÙ„")
            except Exception as e:
                self.log(f"âš ï¸ Gemini: {e}")
        
        self.log(f"ðŸ¤– {len(self.providers)} Ù…Ø²ÙˆØ¯ÙŠ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¬Ø§Ù‡Ø²ÙˆÙ†")
    
    def set_user_key(self, api_key: str):
        """Update Gemini key from user input."""
        if not api_key:
            return
            
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.providers['gemini'] = {
                'model': genai.GenerativeModel('gemini-pro'),
                'name': 'Gemini (User Key)',
                'emoji': 'âœ¨'
            }
            self.log(f"âœ… Gemini Key updated by user")
        except Exception as e:
            self.log(f"âš ï¸ Failed to update Gemini Key: {e}")
    
    def log(self, message: str):
        """Add log message."""
        self.log_messages.append(message)
        print(message)
    
    def get_log(self) -> List[str]:
        """Get all log messages."""
        return self.log_messages
    
    def _call_groq(self, prompt: str) -> str:
        """Call Groq API."""
        try:
            response = self.providers['groq']['client'].chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­ØªØ±Ù. Ø£Ø¬Ø¨ Ø¨Ø¥ÙŠØ¬Ø§Ø² ÙˆÙˆØ¶ÙˆØ­."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Ø®Ø·Ø£: {e}"
    
    def _call_deepseek(self, prompt: str) -> str:
        """Call DeepSeek API."""
        try:
            url = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.providers['deepseek']['key']}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1500
            }
            response = requests.post(url, headers=headers, json=data, timeout=30)
            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            return f"Ø®Ø·Ø£: {e}"
    
    def _call_gemini(self, prompt: str) -> str:
        """Call Gemini API."""
        try:
            response = self.providers['gemini']['model'].generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Ø®Ø·Ø£: {e}"
    
    def _call_all_providers(self, prompt: str) -> Dict[str, str]:
        """Call all providers and collect responses."""
        results = {}
        
        # Use ThreadPoolExecutor for parallel calls
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {}
            
            if 'groq' in self.providers:
                futures[executor.submit(self._call_groq, prompt)] = 'groq'
            if 'deepseek' in self.providers:
                futures[executor.submit(self._call_deepseek, prompt)] = 'deepseek'
            if 'gemini' in self.providers:
                futures[executor.submit(self._call_gemini, prompt)] = 'gemini'
            
            for future in as_completed(futures):
                provider = futures[future]
                try:
                    results[provider] = future.result()
                except Exception as e:
                    results[provider] = f"Ø®Ø·Ø£: {e}"
        
        return results
    
    def _combine_insights(self, responses: Dict[str, str], lang: str = 'ar') -> str:
        """Combine insights from all providers into one comprehensive response."""
        valid_responses = {k: v for k, v in responses.items() if not v.startswith("Ø®Ø·Ø£")}
        
        if not valid_responses:
            return "Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ø£ÙŠ Ù…Ù† Ù…Ø²ÙˆØ¯ÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„."
        
        # If only one provider responded, return its response
        if len(valid_responses) == 1:
            provider = list(valid_responses.keys())[0]
            emoji = self.providers[provider]['emoji']
            return f"{emoji} {valid_responses[provider]}"
        
        # Combine multiple responses
        if lang == 'ar':
            combined = "## ðŸ¤– ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹\n\n"
            
            for provider, response in valid_responses.items():
                emoji = self.providers[provider]['emoji']
                name = self.providers[provider]['name']
                combined += f"### {emoji} {name}:\n{response}\n\n"
            
            combined += "---\n"
            combined += f"*ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© {len(valid_responses)} Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ*"
        else:
            combined = "## ðŸ¤– Combined AI Analysis\n\n"
            
            for provider, response in valid_responses.items():
                emoji = self.providers[provider]['emoji']
                name = self.providers[provider]['name']
                combined += f"### {emoji} {name}:\n{response}\n\n"
            
            combined += "---\n"
            combined += f"*Analyzed by {len(valid_responses)} AI models*"
        
        return combined
    
    def analyze_data_quality(self, analysis: Dict[str, Any], lang: str = 'ar') -> str:
        """Analyze data quality using all AI providers."""
        prompt = f"""
        Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª. Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¥ÙŠØ¬Ø§Ø² (3-4 Ù†Ù‚Ø§Ø· ÙÙ‚Ø·):
        
        - Ø§Ù„ØµÙÙˆÙ: {analysis.get('rows', 0)}
        - Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {analysis.get('columns', 0)}
        - Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ©: {len(analysis.get('numeric_columns', []))}
        - Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ©: {len(analysis.get('categorical_columns', []))}
        - Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©: {analysis.get('total_missing', 0)}
        - Ù…ÙƒØ±Ø±Ø§Øª: {analysis.get('duplicates', 0)}
        
        Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}
        Ø§Ø³ØªØ®Ø¯Ù… Ø¥ÙŠÙ…ÙˆØ¬ÙŠ. ÙƒÙ† Ù…ÙˆØ¬Ø²Ø§Ù‹.
        """
        
        responses = self._call_all_providers(prompt)
        return self._combine_insights(responses, lang)
    
    def generate_insights(self, results: Dict[str, Any], target_col: str, lang: str = 'ar') -> str:
        """Generate strategic insights using all AI providers."""
        # Get top features safely
        feature_importance = results.get('feature_importance')
        if isinstance(feature_importance, pd.DataFrame) and not feature_importance.empty:
            top_features = feature_importance.head(5)['Feature'].tolist()
        else:
            top_features = []
        
        prompt = f"""
        Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙ‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
        
        - Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: {results.get('problem_type', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
        - Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {results.get('best_model', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
        - Ø§Ù„Ø£Ø¯Ø§Ø¡: {results.get('metrics', {})}
        - Ø£Ù‡Ù… Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª: {top_features}
        - Ø§Ù„Ù‡Ø¯Ù: {target_col}
        
        Ù‚Ø¯Ù… 3-4 ØªÙˆØµÙŠØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…ÙˆØ¬Ø²Ø©.
        Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}
        Ø§Ø³ØªØ®Ø¯Ù… Ø¥ÙŠÙ…ÙˆØ¬ÙŠ.
        """
        
        responses = self._call_all_providers(prompt)
        return self._combine_insights(responses, lang)
    
    def suggest_feature_engineering(self, df: pd.DataFrame, lang: str = 'ar') -> str:
        """Suggest feature engineering using all AI providers."""
        columns_info = {col: str(df[col].dtype) for col in list(df.columns)[:15]}
        
        prompt = f"""
        Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‡Ù†Ø¯Ø³Ø© Ù…ÙŠØ²Ø§Øª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:
        {columns_info}
        
        Ø§Ù‚ØªØ±Ø­ 3-4 Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§.
        Ø§Ù„Ù„ØºØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}
        ÙƒÙ† Ù…Ø­Ø¯Ø¯Ø§Ù‹ ÙˆØ¹Ù…Ù„ÙŠØ§Ù‹.
        """
        
        responses = self._call_all_providers(prompt)
        return self._combine_insights(responses, lang)
    
    def understand_data(self, df: pd.DataFrame, target_col: Optional[str] = None) -> Dict[str, Any]:
        """Deep data understanding using ensemble."""
        summary = self._create_data_summary(df, target_col)
        
        prompt = f"""
        Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ£Ø¹Ø·Ù†ÙŠ JSON ÙÙ‚Ø·:
        
        {summary}
        
        {{
            "column_meanings": {{"col": "Ø§Ù„Ù…Ø¹Ù†Ù‰"}},
            "cleaning_strategy": {{"col": "Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©"}},
            "suggested_features": ["Ù…ÙŠØ²Ø©1"],
            "potential_issues": ["Ù…Ø´ÙƒÙ„Ø©1"],
            "recommended_model": "Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"
        }}
        """
        
        # Use Groq for structured responses (fastest and most reliable for JSON)
        if 'groq' in self.providers:
            try:
                response = self._call_groq(prompt)
                result = self._parse_json_response(response)
                result['ai_powered'] = True
                result['ensemble'] = True
                return result
            except:
                pass
        
        # Fallback
        return self._rule_based_understanding(df)
    
    def _create_data_summary(self, df: pd.DataFrame, target_col: Optional[str] = None) -> str:
        """Create data summary."""
        summary = f"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(df)} ØµÙ Ã— {len(df.columns)} Ø¹Ù…ÙˆØ¯\n\nØ§Ù„Ø£Ø¹Ù…Ø¯Ø©:\n"
        
        for col in list(df.columns)[:10]:
            dtype = df[col].dtype
            n_unique = df[col].nunique()
            sample = df[col].dropna().head(2).tolist()
            summary += f"- {col}: {dtype}, ÙØ±ÙŠØ¯={n_unique}, Ø¹ÙŠÙ†Ø©={sample}\n"
        
        if target_col:
            summary += f"\nØ§Ù„Ù‡Ø¯Ù: {target_col}"
        
        return summary
    
    def _parse_json_response(self, text: str) -> Dict:
        """Parse JSON from AI response."""
        try:
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        return {"raw_response": text}
    
    def _rule_based_understanding(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Fallback rule-based understanding."""
        return {
            'ai_powered': False,
            'column_meanings': {col: 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯' for col in df.columns},
            'cleaning_strategy': {},
            'suggested_features': ['Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø¨ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©'],
            'potential_issues': [],
            'recommended_model': 'Random Forest'
        }


# Create singleton instance
_ensemble_instance = None

def get_ensemble() -> AIEnsemble:
    """Get or create the AI Ensemble instance."""
    global _ensemble_instance
    if _ensemble_instance is None:
        _ensemble_instance = AIEnsemble()
    return _ensemble_instance
