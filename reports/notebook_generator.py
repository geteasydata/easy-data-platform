"""
Jupyter Notebook Generator - Creates reproducible analysis notebooks
"""

import json
from typing import Dict, Any, List


def generate_notebook(
    target_col: str,
    problem_type: str,
    feature_importance: List[str],
    metrics: Dict[str, Any],
    cleaning_steps: List[str],
    lang: str = 'ar'
) -> str:
    """
    Generate a comprehensive Jupyter Notebook with all analysis code.
    
    Args:
        target_col: Target column name
        problem_type: 'classification' or 'regression'
        feature_importance: List of top features
        metrics: Model metrics
        cleaning_steps: List of cleaning steps performed
        lang: Language for comments
    
    Returns:
        JSON string of notebook content
    """
    
    notebook = {
        "cells": [],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.9.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    def add_markdown(text: str):
        notebook["cells"].append({
            "cell_type": "markdown",
            "metadata": {},
            "source": text.split("\n")
        })
    
    def add_code(code: str):
        notebook["cells"].append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": code.split("\n")
        })
    
    # Title
    if lang == 'ar':
        add_markdown(f"""# ðŸ¤– ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        
**Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù:** `{target_col}`  
**Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** {'ØªØµÙ†ÙŠÙ' if problem_type == 'classification' else 'Ø§Ù†Ø­Ø¯Ø§Ø± (ØªÙˆÙ‚Ø¹ Ù‚ÙŠÙ…Ø© Ø±Ù‚Ù…ÙŠØ©)'}

Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© AI Expert ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„Ù‡ ÙˆØªØ¹Ø¯ÙŠÙ„Ù‡.
""")
    else:
        add_markdown(f"""# ðŸ¤– AI-Powered Data Analysis

**Target Variable:** `{target_col}`  
**Problem Type:** {problem_type.title()}

This code was auto-generated by AI Expert and can be re-run and modified.
""")
    
    # Imports
    if lang == 'ar':
        add_markdown("## 1. ðŸ“¦ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª")
    else:
        add_markdown("## 1. ðŸ“¦ Import Libraries")
    
    add_code("""import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, r2_score, classification_report, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# For better plots
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)
%matplotlib inline""")
    
    # Load Data
    if lang == 'ar':
        add_markdown("## 2. ðŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    else:
        add_markdown("## 2. ðŸ“‚ Load Data")
    
    add_code("""# Replace with your file path
# Ø§Ø³ØªØ¨Ø¯Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø¨Ù…Ø³Ø§Ø± Ù…Ù„ÙÙƒ - ÙŠÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø°ÙŠ Ù‚Ù…Øª Ø¨ØªØ­Ù…ÙŠÙ„Ù‡
# Load the cleaned data from the Excel report (Sheet: Clean_Data)
try:
    df = pd.read_excel('AI_Expert_Results.xlsx', sheet_name='Clean_Data' if 'Clean_Data' in pd.ExcelFile('AI_Expert_Results.xlsx').sheet_names else 0)
except FileNotFoundError:
    # Fallback to CSV if you have it
    df = pd.read_csv('your_data.csv')

print(f"Data Shape: {df.shape}")
df.head()""")
    
    # EDA
    if lang == 'ar':
        add_markdown("## 3. ðŸ” Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    else:
        add_markdown("## 3. ðŸ” Exploratory Data Analysis")
    
    add_code("""# Basic Info
print("=== Dataset Info ===")
print(df.info())
print("\\n=== Statistical Summary ===")
df.describe()""")
    
    add_code("""# Missing Values
missing = df.isnull().sum()
if missing.sum() > 0:
    print("Missing Values:")
    print(missing[missing > 0])
else:
    print("No missing values!")""")
    
    # Cleaning
    if lang == 'ar':
        add_markdown(f"""## 4. ðŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§:
{chr(10).join(['- ' + step for step in cleaning_steps])}
""")
    else:
        add_markdown(f"""## 4. ðŸ§¹ Data Cleaning

Steps performed:
{chr(10).join(['- ' + step for step in cleaning_steps])}
""")
    
    add_code(f"""# Define target
target = '{target_col}'

# Remove duplicates
df = df.drop_duplicates()

# Handle missing values
# Numeric: fill with median
num_cols = df.select_dtypes(include=np.number).columns
df[num_cols] = df[num_cols].fillna(df[num_cols].median())

# Categorical: fill with mode
cat_cols = df.select_dtypes(exclude=np.number).columns
for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print("Data cleaned!")
print(f"Final shape: {{df.shape}}")""")
    
    # Encoding
    if lang == 'ar':
        add_markdown("## 5. ðŸ·ï¸ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©")
    else:
        add_markdown("## 5. ðŸ·ï¸ Encode Categorical Variables")
    
    add_code(f"""# Encode categorical columns
label_encoders = {{}}
for col in df.select_dtypes(include='object').columns:
    if col != target:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le

# Encode target if categorical
if df[target].dtype == 'object':
    target_le = LabelEncoder()
    df[target] = target_le.fit_transform(df[target])
    print(f"Target classes: {{target_le.classes_}}")""")
    
    # Modeling
    if lang == 'ar':
        add_markdown("## 6. ðŸ§  Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    else:
        add_markdown("## 6. ðŸ§  Build Model")
    
    model_class = "RandomForestClassifier" if problem_type == 'classification' else "RandomForestRegressor"
    
    add_code(f"""# Prepare data
X = df.drop(columns=[target])
y = df[target]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Train model
model = {model_class}(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

print("Model trained successfully!")""")
    
    # Evaluation
    if lang == 'ar':
        add_markdown("## 7. ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    else:
        add_markdown("## 7. ðŸ“Š Evaluate Model")
    
    if problem_type == 'classification':
        add_code("""# Predictions
y_pred = model.predict(X_test)

# Metrics
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2%}")
print("\\nClassification Report:")
print(classification_report(y_test, y_pred))""")
    else:
        add_code("""# Predictions
y_pred = model.predict(X_test)

# Metrics
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RÂ² Score: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")""")
    
    # Feature Importance
    if lang == 'ar':
        add_markdown("## 8. ðŸ”¥ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª")
    else:
        add_markdown("## 8. ðŸ”¥ Feature Importance")
    
    add_code("""# Feature Importance
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importance')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

importance_df.head(10)""")
    
    # Predictions
    if lang == 'ar':
        add_markdown("## 9. ðŸŽ¯ Ø§Ù„ØªÙˆÙ‚Ø¹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©")
    else:
        add_markdown("## 9. ðŸŽ¯ Predict on New Data")
    
    add_code("""# Load new data
# new_data = pd.read_csv('new_data.csv')

# Apply same preprocessing
# new_data_scaled = scaler.transform(new_data)

# Make predictions
# predictions = model.predict(new_data_scaled)

print("Ready for predictions!")""")
    
    # Save Model
    if lang == 'ar':
        add_markdown("## 10. ðŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    else:
        add_markdown("## 10. ðŸ’¾ Save Model")
    
    add_code("""import joblib

# Save model
joblib.dump(model, 'trained_model.pkl')

# Save scaler
joblib.dump(scaler, 'scaler.pkl')

# Save encoders
joblib.dump(label_encoders, 'encoders.pkl')

print("Model and preprocessors saved!")

# To load later:
# model = joblib.load('trained_model.pkl')""")
    
    return json.dumps(notebook, indent=2, ensure_ascii=False)


def get_notebook_bytes(notebook_json: str) -> bytes:
    """Convert notebook JSON to bytes for download."""
    return notebook_json.encode('utf-8')
